# ROADMAP - Paper Agent

> **üìñ Status Atual:** Para entender o estado atual do sistema (√©picos conclu√≠dos, funcionalidades implementadas), consulte [README.md](README.md) e [ARCHITECTURE.md](ARCHITECTURE.md).

> **üìñ Melhorias T√©cnicas:** Para funcionalidades planejadas n√£o vinculadas a √©picos, consulte [docs/backlog.md](docs/backlog.md).

> **üìñ Vis√£o de Produto:** Para entender tipos de artigo, fluxos adaptativos e jornada do usu√°rio, consulte `docs/vision/vision.md`.

---

## üìã Status dos √âpicos

### ‚úÖ Conclu√≠dos
- Infraestrutura base completa
- **√âPICO 1**: Converg√™ncia Org√¢nica
- **√âPICO 2**: Sidebar
- **√âPICO 3**: Bastidores
- **√âPICO 4**: Contexto
- **√âPICO 5**: UX Polish
- **√âPICO 6**: Limpeza de Testes
- **√âPICO 7**: Valida√ß√£o de Maturidade do Sistema - Fase Manual

### üü° √âpicos Em Andamento
- _Nenhum √©pico em andamento no momento_

### ‚è≥ √âpicos Planejados

> **Nota:** √âpicos foram renumerados. O antigo "√âPICO 6: Qualidade de Testes" foi dividido em 3 √©picos refinados (6, 7, 8). √âpicos antigos 7-11 foram renumerados para 9-13.

#### Planejados (refinados)
- **√âPICO 8**: An√°lise Assistida de Qualidade
- **√âPICO 9**: Integra√ß√£o Backend‚ÜîFrontend

#### Planejados (n√£o refinados)
- **√âPICO 10**: Conceitos (n√£o refinado)
- **√âPICO 11**: Alinhamento de Ontologia (n√£o refinado)
- **√âPICO 12**: Pesquisador (n√£o refinado)
- **√âPICO 13**: Escritor (n√£o refinado)


**Regra**: Claude Code s√≥ trabalha em funcionalidades de √©picos refinados.

> Para fluxo completo de planejamento, consulte `planning_guidelines.md`.

---

## √âPICO 5: UX Polish

**Objetivo:** Ajustes de experi√™ncia do usu√°rio: custo em R$.

**Status:** ‚úÖ Conclu√≠do

Custos exibidos em reais (BRL) com formato brasileiro, aplicado em toda interface (chat, backstage, dashboard).

---

## √âPICO 6: Limpeza de Testes

**Objetivo:** Remover testes burocr√°ticos e adicionar testes de integra√ß√£o reais onde h√° mocks superficiais.

**Status:** ‚úÖ Conclu√≠do

Suite de testes limpa e focada: testes burocr√°ticos removidos, testes de integra√ß√£o reais adicionados para validar comportamento do LLM, documenta√ß√£o atualizada com novos padr√µes.

---

## √âPICO 7: Valida√ß√£o de Maturidade do Sistema - Fase Manual

**Objetivo:** Validar que sistema multi-agente funciona como deveria atrav√©s de roteiro de cen√°rios cr√≠ticos executados manualmente.

**Status:** ‚úÖ Conclu√≠do

10 cen√°rios cr√≠ticos executados e validados (10/10 bem-sucedidos). Problema cr√≠tico identificado e corrigido (regra "Turno 1" bloqueava transi√ß√£o autom√°tica). Relat√≥rio completo em `docs/testing/epic7_results/summary.md`.

---

## √âPICO 8: An√°lise Assistida de Qualidade

**Objetivo:** Facilitar an√°lise humana de qualidade conversacional atrav√©s de ferramentas que estruturam dados para discuss√£o eficiente com LLM.

**Status:** ‚è≥ Planejado (refinado)

**Depend√™ncias:** √âpico 7 (precisa identificar problemas reais primeiro)

**Dura√ß√£o estimada:** 6 dias

**Custo estimado:** ~$0.10-0.20 por execu√ß√£o completa (discuss√£o com Claude)

**Consulte:** `docs/testing/epic8_automation_strategy.md` para estrat√©gia completa

### Motiva√ß√£o

**Insight do √âpico 7:**
O valor N√ÉO veio de automa√ß√£o, mas de **discuss√£o contextualizada**:
- Investiga√ß√£o interativa ("me mostre os logs", "por que isso?")
- Decis√µes estrat√©gicas debatidas (baseline opcional?)
- Planejamento adaptativo (pivotamos de manual ‚Üí automatizado)
- **Humano + Claude analisando JUNTOS**

**Problema:**
- Valida√ß√£o manual (√âpico 7) foi eficaz mas trabalhosa (~2-3h)
- Precisamos reduzir tempo de setup
- MAS: Automa√ß√£o completa perde contexto e qualidade

**Solu√ß√£o:**
- Ferramentas que **estruturam dados** para an√°lise
- Output formatado para **f√°cil discuss√£o** com LLM
- Humano + Claude fazem an√°lise (n√£o script)
- **Mant√©m qualidade, reduz trabalho manual**

**Resultado Esperado:**
- Rodar cen√°rio completo: 1 comando
- Gerar relat√≥rio estruturado: autom√°tico
- Colar no Claude e discutir: 30 segundos
- Identificar causa raiz: minutos (n√£o horas)
- **60-75% mais r√°pido que manual, mant√©m qualidade**

---

### Funcionalidades (8.1 - 8.5)

#### 8.1 Multi-Turn Executor (PRIORIDADE #1)

**Objetivo:** Rodar cen√°rios completos end-to-end (3-5 turnos).

**Descri√ß√£o:** 
Implementar executor que valida fluxos multi-agente completos identificados como incompletos no √âpico 7 (Cen√°rios 3, 6, 7). Sistema deve executar conversas turn-by-turn, validar transi√ß√µes entre agentes, e gerar dados estruturados para an√°lise.

**Componentes:**
- `ConversationScenario`: Define cen√°rios multi-turn com turnos esperados
- `MultiTurnExecutor`: Executa cen√°rios e coleta resultados
- `run_scenario.py`: CLI para rodar cen√°rios individuais
- `run_all_scenarios.py`: CLI para rodar suite completa

**Crit√©rios de Aceite:**
- Deve implementar `ConversationScenario` em `utils/test_scenarios.py`
  - Suportar defini√ß√£o de turnos: `[("user", "input"), ("system", "action")]`
  - Suportar agentes esperados: `["orchestrator", "structurer", "methodologist"]`
  - Suportar valida√ß√£o de estado final esperado
- Deve implementar `MultiTurnExecutor` em `scripts/testing/multi_turn_executor.py`
  - Executar cen√°rios turn-by-turn
  - Rastrear agentes chamados em cada transi√ß√£o
  - Preservar estado entre turnos
  - Coletar m√©tricas (tokens, custo, dura√ß√£o)
- Deve converter Cen√°rios 3, 6, 7 do √âpico 7 para formato multi-turn
- Deve implementar `run_scenario.py` com CLI:
  - `--scenario N`: roda cen√°rio espec√≠fico
  - Output estruturado no terminal (sem clipboard)
- Deve implementar `run_all_scenarios.py`:
  - Roda todos os 10 cen√°rios sequencialmente
  - `--save`: salva resultados em JSON para compara√ß√£o
  - Gera sum√°rio de execu√ß√£o
- Deve validar que fluxos completos funcionam (n√£o apenas turno 1)

**Tempo estimado:** 2 dias

---

#### 8.2 Report Generator

**Objetivo:** Formatar dados de forma otimizada para an√°lise com LLM.

**Descri√ß√£o:**
Implementar formatadores que transformam resultados de execu√ß√£o em relat√≥rios estruturados, concisos e otimizados para discuss√£o com Claude. Diferentes formatadores para diferentes focos de an√°lise (transi√ß√µes, contexto, decis√µes).

**Componentes:**
- `format_for_llm_analysis()`: Formata resultados para an√°lise geral
- `format_transition_analysis()`: Foca em transi√ß√µes entre agentes
- `format_context_analysis()`: Foca em preserva√ß√£o de contexto
- `format_decision_analysis()`: Foca em qualidade de decis√µes
- `analyze_scenario.py`: CLI que aplica formatadores

**Crit√©rios de Aceite:**
- Deve implementar formatadores em `utils/report_formatter.py`
  - `format_for_llm_analysis(result, focus="general")`: formata√ß√£o base
  - Suportar focos: "general", "transition", "context", "decision"
  - Output em markdown, conciso e estruturado
  - Incluir se√ß√µes: Contexto, Esperado vs Observado, Logs Relevantes, Problema Detectado, Sugest√£o de An√°lise
- Deve implementar `analyze_scenario.py` com CLI:
  - `--scenario N`: analisa cen√°rio espec√≠fico
  - `--focus [transition|context|decision]`: aplica formatador espec√≠fico
  - Output estruturado no terminal
- Relat√≥rios devem ser < 50 linhas (concisos)
- Deve incluir "Pergunta sugerida para Claude" ao final
- Output deve ser f√°cil de copiar manualmente do terminal

**Tempo estimado:** 1 dia

---

#### 8.3 Comparison Tool

**Objetivo:** Comparar antes/depois de mudan√ßas no prompt para detectar regress√µes.

**Descri√ß√£o:**
Implementar ferramenta que compara resultados de execu√ß√£o antes/depois de mudan√ßas no c√≥digo/prompt, identifica regress√µes automaticamente, e gera relat√≥rio de impacto estruturado para discuss√£o.

**Componentes:**
- `compare_results()`: Compara dois arquivos JSON de resultados
- `detect_regressions()`: Identifica regress√µes automaticamente
- `compare_results.py`: CLI para compara√ß√µes

**Crit√©rios de Aceite:**
- Deve implementar `compare_results()` em `utils/result_comparer.py`
  - Receber dois arquivos JSON (before, after)
  - Identificar cen√°rios que mudaram
  - Classificar mudan√ßas: CORRE√á√ÉO ‚úÖ, REGRESS√ÉO ‚ùå, SEM MUDAN√áA ‚ö™
  - Detectar mudan√ßas em m√©tricas (custo, dura√ß√£o, tokens)
- Deve implementar `detect_regressions()`:
  - Cen√°rio passou ‚Üí falhou: REGRESS√ÉO CR√çTICA üî¥
  - Score caiu >2 pontos: ATEN√á√ÉO üü°
  - Custo aumentou >50%: ATEN√á√ÉO üü°
  - Dura√ß√£o aumentou >100%: ATEN√á√ÉO üü°
- Deve implementar `compare_results.py` com CLI:
  - `--before baseline.json`: arquivo de refer√™ncia
  - `--after current.json`: arquivo atual
  - `--summary`: mostra apenas sum√°rio (n√£o detalhes)
- Output deve incluir:
  - Resumo de mudan√ßas (X corre√ß√µes, Y regress√µes, Z sem mudan√ßa)
  - Lista de cen√°rios que precisam aten√ß√£o
  - Sugest√£o: "Copie acima e pergunte ao Claude: 'Qual cen√°rio priorizar?'"
- Deve salvar diff detalhado em `comparison_report.md` (opcional)

**Tempo estimado:** 1 dia

---

#### 8.4 Interactive Analysis Mode

**Objetivo:** Guiar fluxo de investiga√ß√£o de forma interativa.

**Descri√ß√£o:**
Implementar modo interativo que apresenta menu de op√ß√µes, executa a√ß√µes conforme escolha do usu√°rio, e gera outputs estruturados para discuss√£o. Facilita explora√ß√£o e troubleshooting sem precisar lembrar comandos.

**Componentes:**
- `interactive_analyzer.py`: Script com menu interativo
- Integra√ß√£o com ferramentas existentes (executor, report, comparison, debug)

**Crit√©rios de Aceite:**
- Deve implementar `interactive_analyzer.py` em `scripts/testing/`
- Menu inicial deve listar:
  - 10 cen√°rios dispon√≠veis com nomes descritivos
  - Op√ß√£o para rodar todos os cen√°rios
  - Op√ß√£o para comparar com baseline
  - Op√ß√£o para sair
- Ap√≥s executar cen√°rio, deve oferecer:
  - Ver relat√≥rio formatado
  - Ver debug logs detalhados
  - Comparar com baseline
  - Gerar prompt de corre√ß√£o para Cursor
  - Analisar outro cen√°rio
  - Voltar ao menu principal
- Outputs devem ser printados no terminal (sem clipboard)
- Deve validar inputs do usu√°rio (n√∫meros v√°lidos)
- Deve ter op√ß√£o "voltar" em cada submenu
- Deve ser intuitivo (n√£o requer documenta√ß√£o para usar)

**Tempo estimado:** 1 dia

---

#### 8.5 Debug Workflow

**Objetivo:** Facilitar troubleshooting de problemas sutis com logs detalhados.

**Descri√ß√£o:**
Implementar workflow de debug que gera logs completos (prompt enviado, resposta bruta, reasoning do LLM, decis√µes step-by-step) quando problema √© identificado. Logs formatados para an√°lise com Claude.

**Componentes:**
- `generate_debug_report()`: Gera relat√≥rio de debug detalhado
- `debug_scenario.py`: CLI para debug de cen√°rios espec√≠ficos
- `compare_prompts.py`: Compara comportamento antes/depois de mudan√ßa no prompt (futuro)

**Crit√©rios de Aceite:**
- Deve implementar `generate_debug_report()` em `utils/debug_reporter.py`
  - Incluir prompt COMPLETO enviado ao LLM (sem truncar)
  - Incluir resposta bruta antes de parsing JSON
  - Incluir reasoning do LLM (campo "reasoning")
  - Incluir decis√µes tomadas (next_step, agent_suggestion)
  - Incluir estado antes/depois de cada transi√ß√£o
  - Incluir timestamps e dura√ß√£o de cada opera√ß√£o
- Deve implementar `debug_scenario.py` com CLI:
  - `--scenario N`: gera debug do cen√°rio espec√≠fico
  - `--turn N`: foca em turno espec√≠fico (opcional)
  - `--save`: salva em arquivo `logs/debug/cenario_N_TIMESTAMP.md`
- Debug report deve ser < 100 linhas por turno (conciso mas completo)
- Deve incluir marcadores visuais:
  - `[PROMPT ENVIADO]`: in√≠cio do prompt
  - `[RESPOSTA LLM]`: resposta bruta
  - `[REASONING]`: reasoning extra√≠do
  - `[DECIS√ÉO]`: decis√£o tomada (next_step)
  - `[PROBLEMA]`: onde problema foi detectado (se houver)
- Output deve terminar com:
  - "Copie acima e pergunte ao Claude: 'Onde o reasoning levou √† decis√£o errada?'"
- Logs devem ser salvos automaticamente em `logs/debug/` para hist√≥rico

**Tempo estimado:** 1 dia

---

### Cronograma √âpico 8 (Atualizado)

| Funcionalidade | Dura√ß√£o | Depend√™ncias | Prioridade |
|----------------|---------|--------------|------------|
| 8.1: Multi-Turn Executor | 2 dias | - | üî¥ CR√çTICA |
| 8.2: Report Generator | 1 dia | 8.1 | üî¥ CR√çTICA |
| 8.3: Comparison Tool | 1 dia | 8.1 | üü° ALTA |
| 8.4: Interactive Mode | 1 dia | 8.1, 8.2 | üü° ALTA |
| 8.5: Debug Workflow | 1 dia | 8.1 | üü° ALTA |
| **Total** | **6 dias** | | |

**Mudan√ßas em rela√ß√£o √† proposta original:**
- ‚Üë Dura√ß√£o total: 2-3 ‚Üí 6 dias (+3-4 dias)
- Reordenado: Multi-turn agora √© #1 (era #2)
- Removido: CI/CD (prematuro - postergar)
- Removido: LLM-as-Judge como √∫nica forma de valida√ß√£o
- Adicionado: Ferramentas de an√°lise assistida

---

### Custo Estimado

**Uso das Ferramentas (Sem Custo LLM):**
- Rodar cen√°rio: $0 (apenas execu√ß√£o local)
- Gerar relat√≥rio: $0 (formata√ß√£o de dados)
- Debug logs: $0 (extra√ß√£o de logs)
- Compara√ß√£o: $0 (diff de arquivos JSON)

**Discuss√£o com Claude (Custo LLM):**
- Por problema investigado: ~$0.01-0.02 (5-10 mensagens)
- Suite completa (10 cen√°rios): ~$0.10-0.20 (se todos tiverem problemas)
- Execu√ß√£o semanal: ~$0.10-0.15 (desenvolvimento t√≠pico)

**Comparado com √âpico 7:**
- √âpico 7 manual: ~2-3h de trabalho, $0.13
- √âpico 8 assistido: ~30-45min de trabalho, $0.10-0.20
- **Economia:** 60-75% do tempo, custo similar

---

### Aprendizados do √âpico 7 que Moldaram este √âpico

#### 1. Discuss√£o Contextualizada Gera Mais Valor

**O que funcionou:**
- Investiga√ß√£o interativa com Claude
- An√°lise de logs detalhados
- Decis√µes estrat√©gicas debatidas

**O que N√ÉO teria funcionado:**
- LLM-as-Judge: "Score: 4/5"
- Automa√ß√£o superficial sem contexto

#### 2. Multi-Turn √â Cr√≠tico

**Problema:** Cen√°rios 3 e 6 ficaram incompletos (script single-turn)
**Solu√ß√£o:** Multi-turn executor √© funcionalidade #1

#### 3. Debug Detalhado Foi Essencial

**Problema:** `debug_scenario_2.py` revelou causa raiz
**Solu√ß√£o:** Debug mode embutido no framework

#### 4. Humano Toma Decis√µes Estrat√©gicas

**Exemplos:** Baseline opcional? Cen√°rio mal definido?
**Solu√ß√£o:** LLM assiste, humano decide

#### 5. CI/CD √â Prematuro

**Decis√£o:** Postergar para √âpico 10+ (se necess√°rio)

---

### Documenta√ß√£o

Ap√≥s implementa√ß√£o, deve atualizar:
- `docs/testing/epic8_automation_strategy.md` (j√° reformulado)
- `docs/testing/strategy.md` (adicionar se√ß√£o sobre an√°lise assistida)
- `README.md` (adicionar se√ß√£o sobre ferramentas de an√°lise)

Cada ferramenta deve ter:
- Exemplos de uso em coment√°rios do c√≥digo
- Output de exemplo em docstrings
- Se√ß√£o no README com comandos principais

---

## √âPICO 9: Integra√ß√£o Backend‚ÜîFrontend

**Objetivo:** Completar ciclo de persist√™ncia silenciosa e feedback visual de progresso.

**Status:** ‚úÖ Conclu√≠do

**Depend√™ncias:** Nenhuma

**Dura√ß√£o estimada:** 2-3 dias

### Funcionalidades:

#### 9.1 Atualiza√ß√£o de cognitive_model no Orchestrator ‚úÖ

- **Status:** Conclu√≠do
- **Descri√ß√£o:** Implementar atualiza√ß√£o do cognitive_model no orchestrator_node a cada turno
- **Crit√©rios de Aceite:**
  - Prompt do orchestrator solicita `cognitive_model` no JSON de sa√≠da
  - Orchestrator extrai `cognitive_model` da resposta LLM
  - Orchestrator retorna `cognitive_model` no state update
  - Schema `CognitiveModel` usado para valida√ß√£o (Pydantic)
  - Campos: claim, premises, assumptions, open_questions, contradictions, solid_grounds, context

#### 9.2 Passar active_idea_id via Config ‚úÖ

- **Status:** Conclu√≠do
- **Descri√ß√£o:** Disponibilizar active_idea_id no config do LangGraph (agn√≥stico de framework)
- **Crit√©rios de Aceite:**
  - Streamlit adiciona `active_idea_id` ao config ao invocar grafo
  - Orchestrator acessa `active_idea_id` via `config.get("configurable", {})`
  - Funciona mesmo sem active_idea_id (opcional, n√£o quebra fluxo)

#### 9.3 SnapshotManager no Orquestrador ‚úÖ

- **Status:** Conclu√≠do
- **Descri√ß√£o:** Integrar avalia√ß√£o de maturidade via LLM no orchestrator_node
- **Crit√©rios de Aceite:**
  - Orchestrator chama `create_snapshot_if_mature()` ap√≥s processar turno
  - Usa `SnapshotManager.assess_maturity()` existente (LLM avalia maturidade)
  - Threshold de confian√ßa configur√°vel (padr√£o: 0.8)
  - Silencioso: sem logs vis√≠veis ao usu√°rio, sem notifica√ß√µes
  - Depende de 9.1 (cognitive_model) e 9.2 (active_idea_id)

#### 9.4 Indicador de Solidez no Contexto ‚úÖ

- **Status:** Conclu√≠do
- **Descri√ß√£o:** Exibir barra de progresso de solidez do argumento focal
- **Crit√©rios de Aceite:**
  - Backend: M√©todo reutiliz√°vel calcula solidez (ex: `CognitiveModel.calculate_solidez()`)
  - Frontend: Exibe barra de progresso (0-100%) no painel Contexto
  - Atualiza quando argumento focal muda
  - Agn√≥stico de framework (c√°lculo no backend, UI apenas exibe)

**Ordem de implementa√ß√£o:** 9.1 ‚Üí 9.2 ‚Üí 9.3 ‚Üí 9.4 ‚úÖ

---

## √âPICO 10: Conceitos

**Objetivo:** Criar entidade Concept com vetores sem√¢nticos para busca por similaridade ("produtividade" encontra "efici√™ncia").

**Status:** ‚è≥ Planejado (n√£o refinado)

> **üìñ Filosofia:** Conceitos s√£o ess√™ncias globais (biblioteca √∫nica). Ideias referenciam conceitos, n√£o os possuem. Ver `docs/architecture/ontology.md`.

**Depend√™ncias:**
- √âpico 9

**Consulte:**
- `docs/architecture/concept_model.md` - Schema t√©cnico de Concept
- `docs/architecture/tech_stack.md` - ChromaDB, embeddings, sentence-transformers
- `docs/architecture/ontology.md` - Filosofia: Conceitos como ess√™ncias globais

### Funcionalidades:

#### 10.1 Setup ChromaDB Local [POC]

- **Descri√ß√£o:** Configurar ChromaDB para armazenar vetores sem√¢nticos de conceitos (gratuito, local).
- **Crit√©rios de Aceite:**
  - Deve instalar depend√™ncias: `chromadb`, `sentence-transformers`
  - Deve criar cliente persistente: `chromadb.PersistentClient(path="./data/chroma")`
  - Deve criar collection: `concepts` (metadata: label, essence, variations)
  - Deve usar modelo: `all-MiniLM-L6-v2` (384 dim, 80MB download)

#### 10.2 Schema SQLite de Concept [POC]

- **Descri√ß√£o:** Criar tabelas `concepts` e `idea_concepts` para metadados estruturados e relacionamento N:N.
- **Crit√©rios de Aceite:**
  - Deve criar tabela `concepts`: id, label, essence, variations JSON, chroma_id
  - Deve criar tabela `idea_concepts`: idea_id, concept_id (N:N, PK composta)
  - Campo `chroma_id` deve referenciar registro no ChromaDB
  - Deve criar √≠ndices: ON label, ON idea_id, ON concept_id
  - Conceitos s√£o globais (biblioteca √∫nica), ideias referenciam via `idea_concepts`

#### 10.3 Pipeline de Detec√ß√£o de Conceitos [POC]

- **Descri√ß√£o:** LLM extrai conceitos-chave quando argumento amadurece (ao criar snapshot de Idea) e salva em ChromaDB + SQLite.
- **Crit√©rios de Aceite:**
  - Deve disparar detec√ß√£o ao criar snapshot de Idea (quando argumento amadurece)
  - Deve detectar conceitos via LLM (prompt: "Extrair conceitos-chave desta ideia/argumento")
  - Deve gerar embedding via sentence-transformers
  - Deve salvar no ChromaDB (vetor) + SQLite (metadata)
  - Deve criar registro em `idea_concepts` (linking N:N)
  - **N√£o** deve executar detec√ß√£o a cada mensagem (apenas no snapshot)

#### 10.4 Busca Sem√¢ntica [POC]

- **Descri√ß√£o:** Buscar conceitos similares via embeddings (threshold > 0.80 = mesmo conceito).
- **Crit√©rios de Aceite:**
  - Deve implementar: `find_similar_concepts(query: str, top_k: int) -> list[Concept]`
  - Deve calcular similaridade cosseno entre embeddings
  - Deve usar threshold 0.80 para deduplica√ß√£o ("produtividade" = "efici√™ncia")
  - Deve retornar lista ordenada por similaridade

#### 10.5 Variations Autom√°ticas [Prot√≥tipo]

- **Descri√ß√£o:** Sistema detecta varia√ß√µes lingu√≠sticas e adiciona ao Concept existente (colabora√ß√£o = coopera√ß√£o) com thresholds diferenciados.
- **Crit√©rios de Aceite:**
  - Deve detectar varia√ß√µes via busca sem√¢ntica durante detec√ß√£o de conceitos
  - **Threshold > 0.90:** adicionar variation automaticamente ao Concept existente
  - **Threshold 0.80-0.90:** perguntar ao usu√°rio: "S√£o o mesmo conceito?" (colabora√ß√£o = coopera√ß√£o?)
  - Deve adicionar variation ao Concept existente se confirmado
  - Deve criar novo Concept se usu√°rio rejeitar ou similaridade < 0.80

#### 10.6 Mostrar Conceitos na Interface [Prot√≥tipo]

- **Descri√ß√£o:** Exibir conceitos detectados em dois n√≠veis: preview discreto na p√°gina da ideia + explora√ß√£o completa no Cat√°logo.
- **Crit√©rios de Aceite:**
  - **Preview na p√°gina da ideia** (`/pensamentos/{idea_id}`):
    - Deve mostrar texto discreto: "Usa 3 conceitos: [Coopera√ß√£o] [Fic√ß√£o] [Linguagem]"
    - Tags clic√°veis ‚Üí redireciona para `/catalogo?concept={concept_id}`
  - **Explora√ß√£o completa no Cat√°logo** (`/catalogo`):
    - Deve implementar busca por nome de conceito (LIKE query)
    - Deve implementar filtros: por ideias relacionadas, por variations
    - Deve mostrar lista de ideias que usam o conceito
    - Deve exibir variations como tags secund√°rias
    - Deve permitir navega√ß√£o: conceito ‚Üí ideias relacionadas ‚Üí detalhes da ideia

---

## √âPICO 11: Alinhamento de Ontologia

**Objetivo:** Migrar c√≥digo atual (premises/assumptions como strings separadas) para nova ontologia (Proposi√ß√£o unificada com solidez derivada de Evid√™ncias).

**Status:** ‚è≥ Planejado (n√£o refinado)

**Abordagem:** Evolu√ß√£o gradual, n√£o refatora√ß√£o big-bang.

**Depend√™ncias:**
- √âpicos 9-10 conclu√≠dos

**Refer√™ncias:**
- `docs/architecture/ontology.md` - Nova ontologia
- `docs/vision/epistemology.md` - Fundamentos epistemol√≥gicos

---

## √âPICO 12: Pesquisador

**Objetivo:** Agente para busca e s√≠ntese de literatura cient√≠fica. Introduz Evid√™ncia como entidade pr√°tica.

**Status:** ‚è≥ Planejado (n√£o refinado)

**Depend√™ncias:**
- √âpico 11

---

## √âPICO 13: Escritor

**Objetivo:** Agente para compila√ß√£o de se√ß√µes do artigo cient√≠fico.

**Status:** ‚è≥ Planejado (n√£o refinado)

**Depend√™ncias:**
- √âpico 12

---

## üìù Observa√ß√µes

- Cada √©pico pode ser desenvolvido **isoladamente**
- Entrega **valor incremental**
- Pode ser **testado** antes do pr√≥ximo
- √âpicos n√£o refinados requerem discuss√£o antes da implementa√ß√£o
