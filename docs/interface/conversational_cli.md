# CLI Conversacional - Especifica√ß√£o T√©cnica

**Vers√£o:** 1.0  
**Data:** 15/11/2025  
**Status:** Especifica√ß√£o para Prot√≥tipo (√âpico 7)

## 1. Vis√£o Geral
- CLI com m√∫ltiplos turnos de conversa
- Mant√©m contexto ao longo da sess√£o
- Substitui modelo "input √∫nico ‚Üí fim" por chat cont√≠nuo

## 2. Problema Atual (POC)
**Fluxo atual:**
[CLI inicia]
üìù Digite sua hip√≥tese: tdd reduz bugs
üî¨ Analisando hip√≥tese...
Sistema: "Interessante! Me conte mais..."
üìù Digite sua hip√≥tese: <volta pro in√≠cio, perde contexto>

**Limita√ß√µes:**
- Cada input √© nova sess√£o
- Contexto n√£o preservado
- Orquestrador pede "me conte mais" mas CLI n√£o permite responder

## 3. Solu√ß√£o Proposta (Prot√≥tipo)

**Novo fluxo:**
[CLI inicia]
Sistema: Ol√°! Me conte sobre sua ideia ou observa√ß√£o.
Voc√™: tdd reduz bugs
Sistema: Interessante! Em que contexto voc√™ notou isso?
Voc√™: na minha equipe Python
Sistema: E como mediu redu√ß√£o de bugs?
Voc√™: impress√£o geral, bugs ca√≠ram
Sistema: Vou organizar isso em uma quest√£o de pesquisa estruturada.
[Sistema chama Estruturador automaticamente...]
Sistema: Organizei sua ideia assim: "TDD reduz densidade de bugs em equipes Python?" Isso captura o que voc√™ quer?

## 4. Implementa√ß√£o T√©cnica

### Loop Conversacional

**C√≥digo proposto:**
```python
def run_conversational_cli():
    """Loop conversacional cont√≠nuo."""
    thread_id = f"cli-session-{uuid.uuid4()}"
    
    print("Sistema: Ol√°! Me conte sobre sua ideia ou observa√ß√£o.")
    
    while True:
        # Input do usu√°rio
        user_input = input("\nVoc√™: ").strip()
        
        if user_input.lower() in ['exit', 'sair']:
            print("Sistema: At√© logo!")
            break
        
        # Invocar grafo
        result = graph.invoke(
            {"user_input": user_input},
            config={"configurable": {"thread_id": thread_id}}
        )
        
        # Exibir resposta
        orchestrator_output = result.get('orchestrator_output', {})
        message = orchestrator_output.get('message', '')
        next_step = orchestrator_output.get('next_step', '')
        
        # Transpar√™ncia (opcional)
        if verbose_mode:
            reasoning = orchestrator_output.get('reasoning', '')
            print(f"\nüß† Racioc√≠nio: {reasoning}")
        
        print(f"\nSistema: {message}")
        
        # Decidir pr√≥ximo passo
        if next_step == "suggest_agent":
            # Agente trabalhou nos bastidores, Orquestrador apresentou resultado curado
            # Usu√°rio pode confirmar entendimento ou pedir ajustes
            # (Chamada autom√°tica j√° aconteceu no grafo)
            continue
        
        # Se next_step == "explore", loop continua normalmente
```

### Preserva√ß√£o de Contexto

**Thread ID:**
- Gerado uma vez no in√≠cio da sess√£o
- Passado em toda invoca√ß√£o do grafo
- LangGraph usa para recuperar hist√≥rico completo

**Estado compartilhado:**
- `MultiAgentState` acumula `conversation_history`
- Orquestrador tem acesso a todos os turnos anteriores
- An√°lise contextual considera hist√≥rico completo

## 5. Transpar√™ncia do Racioc√≠nio

**3 n√≠veis de transpar√™ncia:**

### N√≠vel 1: CLI Padr√£o (limpo)
Voc√™: tdd reduz bugs
Sistema: Interessante! Em que contexto voc√™ notou isso?

### N√≠vel 2: CLI Verbose (flag `--verbose`)
Voc√™: tdd reduz bugs
üß† Racioc√≠nio: Input vago sobre TDD. Preciso contexto: onde observou,
como mediu, qual popula√ß√£o. N√£o tenho informa√ß√£o suficiente para
sugerir agente ainda.
Sistema: Interessante! Em que contexto voc√™ notou isso?

### N√≠vel 3: Dashboard (sempre dispon√≠vel)
- EventBus emite eventos com reasoning completo
- Dashboard Streamlit exibe timeline com reasoning de cada turno
- Usa infraestrutura existente do √âpico 5

**Implementa√ß√£o:**
```python
# CLI emite evento
event_bus.publish_agent_started(
    session_id=thread_id,
    agent="orchestrator",
    metadata={"reasoning": orchestrator_output['reasoning']}
)

# Dashboard consome
events = event_bus.get_session_events(thread_id)
for event in events:
    if event['agent'] == 'orchestrator':
        st.write(f"üß† {event['metadata']['reasoning']}")
```

## 6. Detec√ß√£o de Momento Certo

**L√≥gica n√£o-determin√≠stica:**
- LLM julga se tem informa√ß√£o suficiente
- N√£o usa checklist r√≠gida de campos obrigat√≥rios
- Considera qualidade e quantidade de contexto

**Prompt do Orquestrador:**
Analise o hist√≥rico da conversa. Voc√™ tem CONTEXTO SUFICIENTE para
sugerir agente quando:

Conversa acumulou detalhes relevantes (n√£o precisa estar perfeito)
Usu√°rio deu informa√ß√µes que permitem pr√≥ximo passo √∫til
Chamar agente agora agregaria valor (n√£o apenas "seguir protocolo")

Use julgamento baseado em contexto, n√£o checklist r√≠gida.
Se contexto suficiente: next_step = "suggest_agent"
Se precisa mais info: next_step = "explore"

## 7. Crit√©rios de Aceite

**Funcionalidade:**
- [x] CLI mant√©m conversa por N turnos (n√£o volta para prompt inicial)
- [x] Thread ID preservado ao longo da sess√£o
- [x] Orquestrador acessa hist√≥rico completo
- [x] Sistema detecta "momento certo" para sugerir agente
- [x] Usu√°rio pode aceitar ou recusar sugest√£o de chamar agente

**Transpar√™ncia:**
- [x] CLI padr√£o exibe apenas mensagem limpa
- [x] Flag `--verbose` exibe reasoning inline
- [x] EventBus emite eventos com reasoning
- [ ] Dashboard consome e exibe reasoning (validar manualmente)

**Experi√™ncia:**
- [x] Conversa flui naturalmente (sem quebras)
- [x] Sistema faz perguntas relevantes (depende do prompt do Orquestrador)
- [x] Sugest√µes de agentes fazem sentido no contexto (depende do prompt)

**Status:** ‚úÖ Prot√≥tipo Implementado (15/11/2025)

## 8. Comandos de Execu√ß√£o
```bash
# Modo padrao (CLI limpa)
python -m core.tools.cli.chat

# Modo verbose (exibe reasoning)
python -m core.tools.cli.chat --verbose

# Dashboard (reasoning sempre vis√≠vel)
streamlit run app/dashboard.py
```

---

**Refer√™ncias:**
- `docs/orchestration/conversational_orchestrator/` - L√≥gica do Orquestrador
- `ROADMAP.md` - √âpico 7 Prot√≥tipo
- `docs/interface/cli.md` - CLI original (antes do √âpico 7)

