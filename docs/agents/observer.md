# Observador - Mente Anal√≠tica

**Status:** ‚úÖ Implementado (√âpico 10 + 11.4 + 12 + 14 base)
**Vers√£o:** 3.0
**Data:** 09/12/2025

## Resumo

Agente especializado em observar e catalogar a evolu√ß√£o do racioc√≠nio durante conversas. Trabalha **silenciosamente em paralelo** ao Orquestrador, atualizando o CognitiveModel e extraindo conceitos automaticamente.

**Analogia:**
```
Orquestrador = Ator principal (fala, age, decide)
Observador = Testemunha silenciosa (v√™ tudo, cataloga, n√£o interfere)
```

---

## Filosofia de Design

### Analogia: Sistema L√≠mbico (Processamento Interno)

O Observador funciona como o sistema l√≠mbico no c√©rebro:

- **Sente padr√µes antes de verbaliz√°-los**
- **Processa informa√ß√µes em sil√™ncio**
- **Sinaliza quando algo requer aten√ß√£o**
- **N√£o decide a√ß√µes, apenas alerta**

### Princ√≠pio: Leveza e Aten√ß√£o

‚ö° **Observador deve ser LEVE:**

- Mant√©m apenas CognitiveModel em mem√≥ria ativa (~500 tokens)
- N√ÉO mant√©m hist√≥rico completo de turnos
- N√ÉO consulta Memory Agent diretamente
- Processa rapidamente, sinaliza quando necess√°rio

üîç **Observador √© sempre ATENTO:**

- Processa cada turno em tempo real
- Detecta padr√µes, incongru√™ncias, mudan√ßas de foco
- Opera em sil√™ncio (usu√°rio n√£o v√™ processamento)
- Sinaliza Orquestrador quando detecta algo relevante

---

## Mitose do Orquestrador

### Por Que Separar?

**Antes (Orquestrador monol√≠tico):**
- Facilitava conversa E observava racioc√≠nio
- Duas responsabilidades conflitantes
- Complexidade crescente

**Depois (Separa√ß√£o clara):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ORQUESTRADOR               ‚îÇ  ‚îÇ   OBSERVADOR                 ‚îÇ
‚îÇ   (Facilitador)              ‚îÇ  ‚îÇ   (Mente Anal√≠tica)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Facilita conversa          ‚îÇ  ‚îÇ ‚Ä¢ Monitora TODA conversa     ‚îÇ
‚îÇ ‚Ä¢ Negocia caminhos           ‚îÇ  ‚îÇ ‚Ä¢ Atualiza CognitiveModel    ‚îÇ
‚îÇ ‚Ä¢ Apresenta op√ß√µes           ‚îÇ  ‚îÇ ‚Ä¢ Extrai conceitos           ‚îÇ
‚îÇ ‚Ä¢ Provoca reflex√£o           ‚îÇ  ‚îÇ ‚Ä¢ Avalia evolu√ß√£o            ‚îÇ
‚îÇ ‚Ä¢ Consulta Observador ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚Ä¢ Detecta lacunas            ‚îÇ
‚îÇ ‚Ä¢ Decide next_step           ‚îÇ  ‚îÇ ‚Ä¢ Responde consultas ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Responsabilidades

### 1. Manter CognitiveModel Atualizado (Mem√≥ria Ativa)

O Observador mant√©m apenas o CognitiveModel atual em mem√≥ria:

```python
class Observador:
    def __init__(self):
        self.cognitive_model = CognitiveModel()  # √önica coisa em mem√≥ria
        # N√ÉO mant√©m: hist√≥rico de turnos, mensagens antigas, contexto profundo
```

**Conte√∫do do CognitiveModel:**
- Claim atual (e sua solidez)
- Proposi√ß√µes fundamentadoras
- Conceitos identificados
- Focal argument (dire√ß√£o da conversa)
- Contexto essencial (baseline, popula√ß√£o, m√©trica)

**Tamanho t√≠pico**: ~500 tokens (leve, r√°pido de processar)

### 2. Detectar Necessidade de Consulta a Memory

O Observador n√£o consulta Memory diretamente, mas sinaliza o Orquestrador quando detecta:

**A) Incongru√™ncia (poss√≠vel contradi√ß√£o):**
```python
# Turno atual: "Bugs aumentaram"
# CognitiveModel: baseline_bugs = "est√°vel"

if observador.detecta_incongruencia():
    observador.sinalizar_orquestrador({
        "tipo": "incongruencia",
        "contexto": {
            "turno_atual": "bugs aumentaram",
            "cognitive_model": "baseline_bugs=est√°vel"
        },
        "sugestao": "consultar_memory",
        "query_sugerida": "buscar men√ß√µes a 'bugs' nos √∫ltimos 20 turnos"
    })
```

**B) Refer√™ncia a contexto ausente:**
```python
# Usu√°rio menciona "aquela popula√ß√£o" mas CognitiveModel n√£o tem defini√ß√£o

if "aquela popula√ß√£o" in turno and not cognitive_model.tem("popula√ß√£o"):
    observador.sinalizar_orquestrador({
        "tipo": "contexto_ausente",
        "termo": "popula√ß√£o",
        "sugestao": "consultar_memory",
        "query_sugerida": "buscar defini√ß√£o de 'popula√ß√£o'"
    })
```

**C) Mudan√ßa de foco (novo t√≥pico):**
```python
# Foco atual: "bugs"
# Usu√°rio: "E aquela ideia de produtividade?"

if observador.detecta_mudanca_foco():
    observador.sinalizar_orquestrador({
        "tipo": "mudanca_foco",
        "foco_anterior": "bugs",
        "foco_novo": "produtividade",
        "sugestao": "consultar_memory",
        "query_sugerida": "recuperar discuss√£o sobre 'produtividade'"
    })
```

**D) Valida√ß√£o de entendimento:**
```python
# Orquestrador vai perguntar sobre baseline
# Mas usu√°rio pode j√° ter definido

if orquestrador.vai_perguntar("baseline"):
    if not cognitive_model.tem("baseline"):
        observador.sinalizar_orquestrador({
            "tipo": "validacao",
            "termo": "baseline",
            "sugestao": "consultar_memory_primeiro",
            "query_sugerida": "verificar se usu√°rio j√° definiu 'baseline'"
        })
```

**Importante**: Observador **apenas sinaliza**, n√£o decide se consulta ou n√£o. Decis√£o √© do Orquestrador.

### O que FAZ

- ‚úÖ **Monitorar TODA conversa** (todo turno, n√£o apenas snapshots)
- ‚úÖ **Atualizar CognitiveModel completo:**
  - Claims emergentes (proposi√ß√µes centrais)
  - Fundamentos (argumentos de suporte)
  - Contradi√ß√µes (inconsist√™ncias detectadas)
  - Conceitos (ess√™ncias sem√¢nticas - ChromaDB + SQLite)
  - Open questions (lacunas a investigar)
  - Context (dom√≠nio, popula√ß√£o, tecnologia)
- ‚úÖ **Avaliar evolu√ß√£o** de ideias e argumentos
- ‚úÖ **Detectar lacunas** e inconsist√™ncias
- ‚úÖ **Calcular m√©tricas** (solidez, completude)
- ‚úÖ **Responder consultas** do Orquestrador (insights, n√£o comandos)
- ‚úÖ **Publicar eventos** para Dashboard (silencioso)
- ‚úÖ **Identificar necessidades de esclarecimento** (√âpico 14)
- ‚úÖ **Sugerir perguntas contextuais** para tens√µes e gaps (√âpico 14)

### O que N√ÉO FAZ

- ‚ùå Decidir next_step (quem decide: Orquestrador)
- ‚ùå Falar com usu√°rio (quem fala: Orquestrador)
- ‚ùå Negociar caminhos (quem negocia: Orquestrador)
- ‚ùå Interromper fluxo conversacional
- ‚ùå Consultar Memory Agent diretamente (apenas sinaliza necessidade)

---

## Fluxo de Comunica√ß√£o com Orquestrador

### Observador ‚Üí Orquestrador (Sinaliza√ß√£o)

```
Turno atual chega
      ‚Üì
Observador processa
      ‚Üì
Observador atualiza CognitiveModel
      ‚Üì
Observador detecta padr√£o/incongru√™ncia?
      ‚Üì SIM
Observador SINALIZA Orquestrador
{
    "tipo": "incongruencia" | "contexto_ausente" | "mudanca_foco",
    "contexto": {...},
    "sugestao": "consultar_memory" | "perguntar_usuario",
    "query_sugerida": "..."  # se sugest√£o for consultar_memory
}
      ‚Üì
Orquestrador DECIDE:
‚îú‚îÄ Consultar Memory? (usa query_sugerida)
‚îú‚îÄ Perguntar usu√°rio?
‚îî‚îÄ Ignorar? (sinal era falso positivo)
      ‚Üì
[Se consultou Memory]
Memory retorna contexto
      ‚Üì
Orquestrador envia contexto ao Observador
      ‚Üì
Observador PROCESSA contexto
      ‚Üì
Observador atualiza CognitiveModel (se necess√°rio)
      ‚Üì
Observador confirma: "Contexto integrado" ou "Incongru√™ncia resolvida"
```

### Exemplo Completo

**Cen√°rio:** Usu√°rio diz "bugs aumentaram" mas CognitiveModel tinha "bugs est√°veis"

**1. Observador detecta incongru√™ncia:**
```python
observador.sinalizar_orquestrador({
    "tipo": "incongruencia",
    "contexto": {
        "turno_atual": "bugs aumentaram",
        "cognitive_model": "baseline_bugs='est√°vel'"
    },
    "sugestao": "consultar_memory",
    "query_sugerida": "buscar men√ß√µes a 'bugs' nos √∫ltimos 20 turnos"
})
```

**2. Orquestrador decide consultar Memory:**
```python
contexto = memory_agent.query("buscar men√ß√µes a 'bugs' nos √∫ltimos 20 turnos")

# Memory retorna:
# Turno 3: "Bugs est√£o est√°veis h√° 6 meses"
# Turno 15: "Bugs aumentaram 20% no √∫ltimo m√™s"
```

**3. Orquestrador envia contexto ao Observador:**
```python
observador.processar_contexto_memory({
    "turno_3": "Bugs est√°veis h√° 6 meses",
    "turno_15": "Bugs aumentaram 20% no √∫ltimo m√™s"
})
```

**4. Observador processa e resolve:**
```python
# Observador analisa: n√£o √© contradi√ß√£o, s√£o per√≠odos diferentes
observador.atualizar_cognitive_model({
    "baseline_bugs_historico": "est√°vel h√° 6 meses",
    "baseline_bugs_recente": "aumentou 20% no √∫ltimo m√™s",
    "nota": "Mudan√ßa temporal, n√£o contradi√ß√£o"
})

observador.confirmar_orquestrador({
    "status": "resolvido",
    "conclusao": "Per√≠odos diferentes, n√£o h√° contradi√ß√£o"
})
```

---

## Limita√ß√µes e Trade-offs

### Por que Observador N√ÉO mant√©m hist√≥rico completo?

**Problema com hist√≥rico completo:**
```
Conversa com 50 turnos:
‚îú‚îÄ Observador processa 10k tokens a cada turno
‚îú‚îÄ Lat√™ncia: 2-3s por processamento
‚îú‚îÄ Custo: alto (reprocessar hist√≥rico sempre)
‚îî‚îÄ N√£o escala para conversas longas (100+ turnos)
```

**Solu√ß√£o: Observador leve + Memory Agent:**
```
Conversa com 50 turnos:
‚îú‚îÄ Observador processa apenas CognitiveModel (~500 tokens)
‚îú‚îÄ Lat√™ncia: 200-300ms por processamento
‚îú‚îÄ Custo: baixo (apenas estado atual)
‚îú‚îÄ Memory consultado apenas quando necess√°rio (5-10% dos turnos)
‚îî‚îÄ Escala para conversas infinitas
```

### O que acontece se Observador perder contexto?

**Cen√°rio**: Usu√°rio menciona algo do passado que n√£o est√° em CognitiveModel

**Sem Memory Agent:**
```
‚ùå Observador n√£o tem acesso ‚Üí Orquestrador pergunta de novo ao usu√°rio
‚ùå Usu√°rio frustrado: "J√° falei isso antes!"
```

**Com Memory Agent:**
```
‚úÖ Observador detecta contexto ausente ‚Üí sinaliza Orquestrador
‚úÖ Orquestrador consulta Memory ‚Üí recupera contexto
‚úÖ Observador integra contexto ‚Üí continua processamento
‚úÖ Usu√°rio nem percebe que sistema "esqueceu"
```

### Trade-off: Velocidade vs Contexto

| Aspecto | Observador Leve | Observador Pesado |
|---------|-----------------|-------------------|
| **Mem√≥ria ativa** | ~500 tokens | ~10k tokens |
| **Lat√™ncia** | 200-300ms | 2-3s |
| **Custo por turno** | Baixo | Alto |
| **Contexto imediato** | Limitado (CognitiveModel) | Completo (hist√≥rico) |
| **Escalabilidade** | Alta (conversas infinitas) | Baixa (quebra em 100+ turnos) |
| **Recupera√ß√£o de contexto** | Via Memory Agent | J√° tem tudo |

**Decis√£o**: Observador Leve + Memory Agent = melhor trade-off

---

## Timing: Todo Turno (Sempre)

**Decis√£o:** Observador processa **TODOS os turnos**, n√£o apenas snapshots.

**Por qu√™?**
- Garante que nada √© perdido
- CognitiveModel sempre atualizado
- Conceitos detectados continuamente
- N√£o depende de eventos externos (snapshots)

**Custo vs Completude:**
- ‚úÖ Completude m√°xima (nunca perde conceito)
- ‚ö†Ô∏è Custo constante (LLM em todo turno)
- ‚ö†Ô∏è Mas: Observador usa modelo eficiente (Haiku) e processamento √© r√°pido

---

## CognitiveModel Completo

### Estrutura Atualizada pelo Observador

> **√âpico 11.4:** Estrutura migrada de `premises`/`assumptions` para `proposicoes` unificadas.

```python
CognitiveModel:
  # Claim (afirma√ß√£o central)
  claim: str

  # Proposi√ß√µes (fundamentos com solidez vari√°vel)
  # Substitui distin√ß√£o bin√°ria premise/assumption
  proposicoes: list[Proposicao]  # {texto, solidez, evidencias}

  # Contradi√ß√µes (inconsist√™ncias)
  contradictions: list[Contradiction]  # {description, confidence, suggested_resolution}

  # Conceitos (ess√™ncias sem√¢nticas)
  concepts_detected: list[str]  # Labels detectados (ChromaDB)

  # Open questions (lacunas)
  open_questions: list[str]

  # Context (contexto evolutivo)
  context: dict  # {domain, population, technology}

  # Solid grounds (evid√™ncias bibliogr√°ficas - futuro)
  solid_grounds: list[SolidGround]  # {claim, evidence, source}
```

**Proposicao:**
```python
class Proposicao:
    texto: str          # Enunciado da proposi√ß√£o
    solidez: float|None # 0-1 (None = n√£o avaliada)
    evidencias: list    # IDs de evid√™ncias (futuro)
```

### Atualiza√ß√£o a Cada Turno

```python
def process_turn(user_input: str, conversation_history: list):
    """Observador processa cada turno (√âpico 11.4)."""

    # 1. Extra√ß√£o sem√¢ntica via LLM
    extracted = extract_all(user_input, conversation_history)
    # extracted = {
    #     "claims": [...],
    #     "concepts": [...],
    #     "proposicoes": [Proposicao(texto=..., solidez=None), ...],
    #     "contradictions": [...],
    #     "open_questions": [...]
    # }

    # 2. Mescla com CognitiveModel anterior
    cognitive_model = merge_cognitive_model(previous, extracted)
    # proposicoes acumulam por similaridade de texto

    # 3. Calcula m√©tricas (via LLM)
    metrics = calculate_metrics(
        claim=cognitive_model["claim"],
        proposicoes=cognitive_model["proposicoes"],  # Lista unificada
        open_questions=cognitive_model["open_questions"],
        contradictions=cognitive_model["contradictions"]
    )

    # 4. Persiste conceitos no cat√°logo
    persist_concepts_batch(extracted["concepts"], idea_id)

    # 5. Publica eventos (silencioso)
    event_bus.publish(CognitiveModelUpdatedEvent(cognitive_model, metrics))
```

---

## Interface de Consulta (N√£o-Determin√≠stica)

### Filosofia

**N√ÉO √© command & control:**
```python
# ‚ùå ERRADO: Observador d√° ordens
solidez = observador.get_solidez()
if solidez < 0.7:
    next_step = "explore"  # Orquestrador perde autonomia
```

**√â di√°logo contextual:**
```python
# ‚úÖ CERTO: Observador d√° insights
insight = observador.what_do_you_see(
    context="Usu√°rio mudou de dire√ß√£o",
    question="Conceitos anteriores ainda relevantes?"
)
# Retorna: {
#   "relevance": "Parcial - LLMs ainda central, mas bugs √© novo foco",
#   "suggestion": "Pode conectar: bugs como m√©trica de produtividade",
#   "confidence": 0.8
# }

# Orquestrador decide autonomamente baseado em insight
decision = decide_with_insight(my_analysis, insight)
```

### Quando Orquestrador Consulta?

**Gatilhos naturais** (n√£o regras fixas):

1. **Mudan√ßa de dire√ß√£o detectada:**
   ```
   Usu√°rio (turno 1): "LLMs aumentam produtividade"
   Usu√°rio (turno 5): "Na verdade, quero focar em bugs"
   
   Orquestrador: "Hmm, percebi mudan√ßa. Deixa eu verificar contexto..."
   ‚Üí Consulta: "O que mudou? Conceitos anteriores ainda relevantes?"
   ```

2. **Contradi√ß√£o aparente:**
   ```
   Usu√°rio (turno 3): "Claude Code √© mais r√°pido"
   Usu√°rio (turno 8): "Mas velocidade n√£o importa tanto"
   
   Orquestrador: "Voc√™ mencionou velocidade antes de forma diferente..."
   ‚Üí Consulta: "H√° contradi√ß√£o? Claim evoluiu ou h√° inconsist√™ncia?"
   ```

3. **Incerteza sobre profundidade:**
   ```
   Usu√°rio: "Produtividade depende de muitos fatores"
   
   Orquestrador: "Quantos fundamentos j√° temos? Vale aprofundar mais?"
   ‚Üí Consulta: "Fundamentos atuais cobrem o claim?"
   ```

4. **Checagem de completude:**
   ```
   Orquestrador: "Acho que temos claim + fundamentos s√≥lidos..."
   ‚Üí Consulta: "Solidez suficiente? H√° gaps cr√≠ticos?"
   ```

### API de Consulta

```python
class ObservadorAPI:
    def what_do_you_see(self, context: str, question: str) -> dict:
        """
        Responde consulta contextual do Orquestrador.
        
        Args:
            context: Contexto da consulta (ex: "mudan√ßa de dire√ß√£o")
            question: Pergunta espec√≠fica (ex: "conceitos ainda relevantes?")
            
        Returns:
            {
                "insight": str,         # Observa√ß√£o principal
                "suggestion": str,      # Sugest√£o de a√ß√£o (opcional)
                "confidence": float,    # 0-1
                "evidence": dict        # Dados do CognitiveModel que sustentam
            }
        """
        pass
    
    def get_current_state(self) -> dict:
        """
        Retorna estado atual completo do CognitiveModel.
        
        Usado quando Orquestrador precisa de vis√£o geral,
        n√£o apenas insight espec√≠fico.
        """
        return cognitive_model.to_dict()
    
    def has_contradiction(self) -> bool:
        """Check r√°pido: h√° contradi√ß√µes detectadas?"""
        return len(cognitive_model.contradictions) > 0
    
    def get_solidez(self) -> float:
        """Check r√°pido: solidez geral atual."""
        return cognitive_model.solidez_geral
```

---

## Visualiza√ß√£o nos Bastidores

### Layout (Ambos Colaps√°veis)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     üìä BASTIDORES                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  [‚ñ∂ Timeline] (colapsado)      ‚îÇ  [‚ñ∂ Observador] (colapsado)   ‚îÇ
‚îÇ                                ‚îÇ                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Estado padr√£o:** Ambos colapsados (interface limpa)

### Timeline (Esquerda - Colaps√°vel)

**Quando expandido:**
```
[‚ñº Timeline]
12:34:01 üéØ Orquestrador analisa input
12:34:02 üéØ Orquestrador consulta Observador
12:34:03 üëÅÔ∏è Observador atualizou modelo (2 conceitos novos)
12:34:04 üéØ Orquestrador decide: explore
12:34:05 üìê Estruturador estrutura quest√£o
```

**Quando mostrar Observador na timeline?**

Apenas quando **relevante:**
- ‚úÖ Conceito novo detectado: "üëÅÔ∏è Observador detectou 2 conceitos: LLMs, Produtividade"
- ‚úÖ Contradi√ß√£o detectada: "üëÅÔ∏è Observador detectou contradi√ß√£o entre X e Y"
- ‚úÖ Solidez mudou significativamente: "üëÅÔ∏è Solidez aumentou: 0.65 ‚Üí 0.80"
- ‚ùå Atualiza√ß√£o rotineira sem novidades

### Painel Observador (Direita - Colaps√°vel)

**Quando expandido:**
```
[‚ñº Observador - Mente Anal√≠tica]

üìã Estado atual do racioc√≠nio:

Conceitos detectados:
‚Ä¢ LLMs (agora)
‚Ä¢ Produtividade (2 turnos atr√°s)
‚Ä¢ Claude Code (agora) üÜï

Claims atuais:
‚Ä¢ "LLMs aumentam produtividade"

Solidez geral: 0.65 ‚ö†Ô∏è

Open questions:
‚Ä¢ Como medir produtividade?
‚Ä¢ Qual popula√ß√£o-alvo?

[‚ñº Ver reasoning completo]
  (expande para mostrar prompt usado, an√°lise LLM)
```

**Informa√ß√µes vis√≠veis:**
- Conceitos detectados (com timing)
- Claims atuais
- Solidez geral (visual: üü¢ alta, üü° m√©dia, üî¥ baixa)
- Open questions pendentes
- Contradi√ß√µes (se houver)

**Modo debug (colaps√°vel dentro do painel):**
- Prompt completo enviado ao LLM
- Resposta bruta do LLM
- Reasoning detalhado
- Embeddings gerados (ChromaDB)

---

## Extra√ß√£o de Conceitos

### Pipeline

```python
def extract_concepts(user_input: str) -> list[Concept]:
    """
    Extrai conceitos-chave do turno atual.
    
    Pipeline:
    1. LLM extrai conceitos (prompt espec√≠fico)
    2. Gera embeddings (sentence-transformers)
    3. Busca similares no cat√°logo (ChromaDB)
    4. Deduplica ou cria novo (threshold 0.80)
    5. Salva metadados (SQLite)
    """
    
    # 1. LLM extrai
    concepts_text = llm.invoke(EXTRACT_CONCEPTS_PROMPT.format(input=user_input))
    
    # 2. Para cada conceito
    for concept_label in concepts_text:
        # Gera embedding
        embedding = sentence_transformer.encode(concept_label)
        
        # Busca similares
        similar = chromadb.query(embedding, top_k=3)
        
        # Deduplica ou cria
        if similar and similar[0].similarity > 0.80:
            # Mesmo conceito, adiciona variation
            concept = similar[0]
            concept.variations.append(concept_label)
        else:
            # Conceito novo
            concept = Concept(
                label=concept_label,
                embedding=embedding
            )
            chromadb.save(concept)
            sqlite.save(concept)
    
    return concepts
```

### Deduplica√ß√£o (Threshold)

- **> 0.80:** Mesmo conceito (adiciona como variation)
- **0.75-0.80:** Zona cinzenta (pergunta ao usu√°rio no futuro)
- **< 0.75:** Conceito diferente (cria novo)

### Exemplo

```
Turno 1: "LLMs aumentam produtividade"
‚Üí Detecta: ["LLMs", "Produtividade"]
‚Üí Salva ambos no cat√°logo

Turno 3: "Language models s√£o eficientes"
‚Üí Detecta: ["Language models", "Efici√™ncia"]
‚Üí "Language models" similar a "LLMs" (0.92)
‚Üí Adiciona "Language models" como variation de "LLMs"
‚Üí "Efici√™ncia" similar a "Produtividade" (0.85)
‚Üí Adiciona "Efici√™ncia" como variation de "Produtividade"

Cat√°logo final:
‚Ä¢ LLMs (variations: ["Language models"])
‚Ä¢ Produtividade (variations: ["Efici√™ncia"])
```

---

## Integra√ß√£o com CognitiveModel

### Rela√ß√£o com √âpico 9 (Snapshots)

**√âpico 9:** Snapshots de Idea (quando argumento amadurece)
**√âpico 10:** Observador processa TODOS os turnos

**Complementaridade:**
- Snapshots = marcos importantes (salva progresso)
- Observador = monitoramento cont√≠nuo (cataloga conceitos)

**Fluxo:**
```
Turno 1-5: Observador cataloga conceitos
Turno 5: Argumento amadurece ‚Üí Snapshot criado
         ‚Üí Snapshot referencia conceitos catalogados
Turno 6-10: Observador continua catalogando
Turno 10: Novo snapshot ‚Üí referencia conceitos novos
```

### Atualiza√ß√£o do Snapshot

```python
# Quando snapshot √© criado
def create_snapshot(idea_id: UUID):
    # Pega conceitos detectados pelo Observador
    conceitos = cognitive_model.conceitos
    
    # Associa ao snapshot
    snapshot = Snapshot(
        idea_id=idea_id,
        concept_ids=conceitos,  # Refer√™ncia N:N
        focal_argument=cognitive_model.claims[0],
        solidez=cognitive_model.solidez_geral
    )
    
    db.save(snapshot)
```

---

## Tecnologias

### LLM
- **Modelo:** claude-3-5-haiku-20241022
- **Justificativa:** Custo-efetivo, r√°pido, suficiente para extra√ß√£o
- **Temperature:** 0 (determin√≠stico)

### ChromaDB
- **Tipo:** Local, persistente
- **Path:** `./data/chroma`
- **Collection:** `concepts`
- **Embedding:** sentence-transformers (all-MiniLM-L6-v2, 384 dim)

### SQLite
- **Tabelas:**
  - `concepts` (id, label, essence, variations JSON, chroma_id)
  - `concept_variations` (concept_id, variation)
  - `idea_concepts` (idea_id, concept_id)

---

## Evolu√ß√£o (√âpicos)

### ‚úÖ √âpico 10: Observador - Mente Anal√≠tica (POC) - COMPLETO
- ‚úÖ **10.1 Mitose do Orquestrador** - IMPLEMENTADO
  - Estrutura `agents/observer/` criada
  - ObservadorAPI com interface de consulta
  - Separa√ß√£o de responsabilidades documentada
- ‚úÖ **10.2 Processamento via LLM** - IMPLEMENTADO
  - Extratores sem√¢nticos (claims, concepts, fundamentos, contradictions)
  - M√©tricas via LLM (solidez, completude)
  - `process_turn()` + `ObserverProcessor`
  - `CognitiveModelUpdatedEvent` no EventBus
- ‚úÖ **10.3 ChromaDB + SQLite setup** - IMPLEMENTADO
  - ChromaDB persistente com cosine distance (`data/chroma/`)
  - SQLite com tabelas: concepts, concept_variations, idea_concepts
  - Embedding model: all-MiniLM-L6-v2 (384 dim)
  - `ConceptCatalog` com deduplica√ß√£o autom√°tica
- ‚úÖ **10.4 Pipeline de conceitos** - IMPLEMENTADO
  - `persist_concepts()` e `persist_concepts_batch()`
  - Integra√ß√£o com `process_turn()` via `persist_concepts_flag`
  - Link N:N entre Idea e Concept via `idea_id`
  - Par√¢metros opcionais para Agentic RAG (Epic 12)
- ‚úÖ **10.5 Busca sem√¢ntica** - IMPLEMENTADO
  - `find_similar_concepts()` com threshold configur√°vel
  - Similaridade cosseno ordenada descendente
  - Thresholds: 0.80 (mesmo conceito), 0.90 (auto-variation)
- ‚úÖ **10.6 Testes POC** - IMPLEMENTADO
  - 22 testes unit√°rios em `tests/unit/test_observer.py`
  - Cobertura: ConceptCatalog, Pipeline, Embeddings, Deduplica√ß√£o
  - Mocks para LLM, vetores fixos para busca sem√¢ntica
### ‚úÖ √âpico 12: Observador Integrado ao Fluxo - COMPLETO

Observer integrado ao grafo multi-agente via callback ass√≠ncrono.

**Implementa√ß√£o:**
- ‚úÖ **12.1 Callback Ass√≠ncrono** - IMPLEMENTADO
  - `_create_observer_callback()` em `agents/multi_agent_graph.py`
  - Thread daemon ap√≥s `orchestrator_node` (n√£o bloqueia shutdown)
  - Atualiza `state["cognitive_model"]` com an√°lise sem√¢ntica
  - Tempo de processamento: <3s em background
  - Publica `CognitiveModelUpdatedEvent` via EventBus

- ‚úÖ **12.2 CognitiveModel no Prompt** - IMPLEMENTADO
  - `_build_cognitive_model_context()` em `agents/orchestrator/nodes.py`
  - Formata claim, proposi√ß√µes (top 5 por solidez), conceitos (max 10)
  - Inclui contradi√ß√µes (max 3), quest√µes abertas (max 5), m√©tricas
  - Orquestrador usa naturalmente via prompt context

- ‚úÖ **12.3 Timeline Visual** - IMPLEMENTADO
  - `render_observer_section()` em `app/components/backstage/timeline.py`
  - Se√ß√£o colaps√°vel "üëÅÔ∏è Observador" com √∫ltimos turnos
  - M√©tricas: conceitos, proposi√ß√µes, solidez, maturidade
  - Modal "üëÅÔ∏è An√°lise do Observador" com hist√≥rico completo

- ‚úÖ **12.4 Testes** - IMPLEMENTADO
  - 9 testes em `tests/unit/test_observer_callback.py`
  - 19 testes em `tests/unit/agents/orchestrator/test_cognitive_context.py`
  - Script `scripts/validate_observer_integration.py`

**Fluxo de integra√ß√£o:**
```
User Input ‚Üí Orchestrator ‚Üí Response
                  ‚Üì
           [Background]
                  ‚Üì
             Observer ‚Üí cognitive_model
                  ‚Üì
         EventBus ‚Üí Timeline
```

**Detalhes:** Ver `docs/epics/epic-12-observer-integration.md`

### üîÑ √âpico 14: Observer - Consultas Inteligentes (Base Implementada)

Sistema de consultas inteligentes que identifica quando o argumento precisa de esclarecimento e sugere perguntas contextuais.

**Filosofia:**
- Observer identifica **O QUE** precisa esclarecimento
- Orquestrador formula perguntas **NATURAIS** (n√£o rob√≥ticas)
- Tom de **parceiro pensante**, n√£o fiscalizador
- Perguntas ajudam a **AVAN√áAR**, n√£o apenas apontar problemas
- Contradi√ß√µes s√£o **tens√µes epistemol√≥gicas**, n√£o erros

**Implementa√ß√£o base:**

- ‚úÖ **14.1 Identifica√ß√£o de Necessidades** - IMPLEMENTADO
  - `identify_clarification_needs()` analisa CognitiveModel
  - Detecta contradi√ß√µes, gaps, confus√£o, mudan√ßas de dire√ß√£o
  - Retorna `ClarificationNeed` com descri√ß√£o e sugest√£o de abordagem

- ‚úÖ **14.3 Perguntas sobre Contradi√ß√µes** - IMPLEMENTADO
  - `generate_contradiction_question()` gera perguntas
  - Explora contextos diferentes (n√£o aponta erro)
  - Prompts especializados para tens√µes epistemol√≥gicas

- ‚úÖ **14.4 Perguntas sobre Gaps** - IMPLEMENTADO
  - `suggest_question_for_gap()` sugere perguntas para lacunas
  - Foco em avan√ßar o argumento, n√£o coletar dados
  - Perguntas espec√≠ficas ao contexto da conversa

- ‚úÖ **14.5 Timing de Interven√ß√£o** - IMPLEMENTADO
  - `should_ask_clarification()` decide QUANDO perguntar
  - Regras: n√£o interromper fluxo, esperar persist√™ncia
  - Contradic√£o 2+ turnos ‚Üí perguntar
  - Usu√°rio fluindo bem ‚Üí n√£o interromper

- ‚úÖ **14.6 An√°lise de Resposta** - IMPLEMENTADO
  - `analyze_clarification_response()` analisa esclarecimento
  - Status: resolved, partially_resolved, unresolved
  - Sugere atualiza√ß√µes no CognitiveModel
  - Eventos: `ClarificationRequestedEvent`, `ClarificationResolvedEvent`

**Pendente (depende do √âpico 13):**
- üîÑ **Integra√ß√£o com Orquestrador** - √âpico 13 define como Observer comunica com Orquestrador
- üîÑ **Timeline visual** - Eventos de clarification na timeline

**M√≥dulos:**
```
agents/observer/
‚îú‚îÄ‚îÄ clarification.py          # Fun√ß√µes principais
‚îú‚îÄ‚îÄ clarification_prompts.py  # Prompts especializados
agents/models/
‚îú‚îÄ‚îÄ clarification.py          # Modelos Pydantic
utils/
‚îú‚îÄ‚îÄ event_models.py           # Eventos de clarification
‚îú‚îÄ‚îÄ event_bus/publishers.py   # M√©todos publish_*
```

**Exemplo de uso:**
```python
from agents.observer import (
    identify_clarification_needs,
    should_ask_clarification,
    generate_contradiction_question
)

# 1. Identificar necessidade
need = identify_clarification_needs(cognitive_model, turn_number=5)

if need.needs_clarification:
    # 2. Decidir timing
    decision = should_ask_clarification(need, turn_history, current_turn=5)

    if decision.should_ask:
        # 3. Gerar pergunta
        if need.clarification_type == "contradiction":
            suggestion = generate_contradiction_question(
                contradiction, propositions, context
            )
            # suggestion.question_text = "Voc√™ mencionou X e Y.
            #   Eles se aplicam em situa√ß√µes diferentes?"
```

### √âpico 13: Cat√°logo de Conceitos (Interface)
- ‚úÖ P√°gina `/catalogo` (busca, filtros, analytics)
- ‚úÖ Preview na p√°gina da ideia
- ‚úÖ Navega√ß√£o: conceito ‚Üí ideias ‚Üí detalhes
- ‚úÖ Export/import de biblioteca

---

## Refer√™ncias

- `docs/architecture/observer_architecture.md` - Arquitetura t√©cnica
- `docs/architecture/ontology.md` - CognitiveModel e MemoryLayer
- `docs/architecture/concept_model.md` - Schema de Concept
- `core/docs/vision/cognitive_model/core.md` - Fundamentos epistemol√≥gicos
- `docs/agents/memory_agent.md` - Consultado via Orquestrador quando necess√°rio
- `docs/agents/orchestrator.md` - Recebe sinaliza√ß√µes do Observador
- `ROADMAP.md` - √âpicos 10, 12, 13

---

**Vers√£o:** 3.0
**Data:** 09/12/2025
**Status:** ‚úÖ √âpico 10-12 Completos | üîÑ √âpico 14 Base (Consultas Inteligentes) | Pendente: Integra√ß√£o com Orquestrador (√âpico 13)

