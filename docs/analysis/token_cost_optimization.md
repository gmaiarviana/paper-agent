# An√°lise: Otimiza√ß√£o de Custos de Tokens

**Data:** 2025-01-27  
**Contexto:** An√°lise de oportunidades de economia de custos de tokens baseada na vis√£o do produto  
**√öltima atualiza√ß√£o:** Revis√£o completa do c√≥digo atual para identificar status de implementa√ß√£o

---

## üìã Resumo Executivo

**Oportunidades Cr√≠ticas de Otimiza√ß√£o:**

1. ‚ö†Ô∏è **Hist√≥rico n√£o truncado no Orquestrador** - Envia todas as mensagens a cada turno
2. ‚ö†Ô∏è **max_tokens n√£o aplicado** - Respostas podem ser mais longas que necess√°rio
3. ‚ö†Ô∏è **JSON indentado em contextos** - Adiciona ~30% de tokens desnecess√°rios
4. ‚ö†Ô∏è **Prompt muito longo** - ~615 linhas com exemplos redundantes

**Economia Potencial:** 40-60% em conversas longas (>10 turnos) com implementa√ß√£o da Fase 1.

---

## üéØ Vis√£o do Produto: Foco Estrat√©gico

**Princ√≠pio central:** "Lapidar UMA ideia por conversa" (n√£o assistente gen√©rico)

Esta filosofia permite otimiza√ß√µes agressivas que n√£o seriam poss√≠veis em sistemas generalistas:

- ‚úÖ **Foco estreito** = menos contexto necess√°rio
- ‚úÖ **Uma ideia por sess√£o** = hist√≥rico pode ser mais agressivamente truncado
- ‚úÖ **Dial√©tica socr√°tica** = respostas curtas e provocativas (n√£o explica√ß√µes longas)
- ‚úÖ **Agentes especializados** = cada um pode ter limites espec√≠ficos

---

## üí∞ Custos Atuais (USD por 1M tokens)

| Modelo | Input | Output | Ratio Output/Input |
|--------|-------|--------|-------------------|
| **Haiku** | $0.80 | $4.00 | 5x |
| **Sonnet** | $3.00 | $15.00 | 5x |
| **Opus** | $15.00 | $75.00 | 5x |

**Insight cr√≠tico:** Output √© 5x mais caro que input. Reduzir tokens de sa√≠da tem impacto 5x maior.

---

## üîç An√°lise de Oportunidades

### 1. **Hist√≥rico de Conversas Cresce Indefinidamente** ‚ö†Ô∏è CR√çTICO

**Problema atual:**
```python
# agents/orchestrator/nodes.py:600
messages = state.get("messages", [])
if messages:
    context_parts.append("HIST√ìRICO DA CONVERSA:")
    for msg in messages:  # ‚ùå TODAS as mensagens, sem limite
        context_parts.append(f"[Usu√°rio]: {msg.content}")
```

**Impacto:**
- Conversas longas enviam todo o hist√≥rico a cada chamada
- 20 turnos = ~10k tokens de hist√≥rico repetido
- Custo acumula exponencialmente

**Solu√ß√£o recomendada:**
1. **Truncamento inteligente:** √öltimas N mensagens + resumo do restante
2. **Resumo incremental:** A cada 10 turnos, resumir mensagens antigas
3. **Foco no argumento focal:** Usar `focal_argument` como contexto principal

**Economia estimada:** 30-50% em conversas longas (>10 turnos)

---

### 2. **Prompt do Orquestrador √© Muito Longo** üìù M√âDIO IMPACTO

**Problema atual:**
```python
# utils/prompts/orchestrator.py
ORCHESTRATOR_SOCRATIC_PROMPT_V1 = """Voc√™ √© o Orquestrador Socr√°tico...
[~615 linhas de prompt]
"""
```

**An√°lise:**
- Prompt tem ~615 linhas (~15k tokens)
- M√∫ltiplos exemplos (7 exemplos completos)
- Instru√ß√µes repetidas em diferentes se√ß√µes
- Formato com muitas linhas em branco

**Oportunidades:**
1. **Consolidar exemplos:** Manter apenas 2-3 melhores (reduzir ~40%)
2. **Remover redund√¢ncias:** Instru√ß√µes repetidas sobre provoca√ß√£o socr√°tica
3. **Formato mais denso:** Reduzir linhas em branco desnecess√°rias
4. **Se√ß√µes opcionais:** Mover exemplos detalhados para refer√™ncia externa

**Economia estimada:** 20-30% em tokens de input do prompt base (~3-4.5k tokens)

---

### 3. **JSON Indentado em Contextos** üìä M√âDIO IMPACTO

**Problema atual:**
```python
# agents/orchestrator/nodes.py:633
context_parts.append(json.dumps(structurer_output, indent=2, ensure_ascii=False))
```

**Oportunidade:**
- `indent=2` adiciona ~30% de tokens (espa√ßos/linhas)
- Para curadoria, formato compacto √© suficiente
- Manter indent apenas para logs/debugging

**Economia estimada:** 5-10% em tokens de input

---

### 4. **Respostas do Orquestrador N√£o T√™m max_tokens** ‚ö†Ô∏è ALTO IMPACTO

**Problema atual:**
```python
# agents/orchestrator/nodes.py:778-780
llm = create_anthropic_client(model=model_name, temperature=0)
messages = [HumanMessage(content=conversational_prompt)]
response = invoke_with_retry(llm=llm, messages=messages, agent_name="orchestrator")
# ‚ùå max_tokens n√£o est√° sendo passado, mesmo com limite definido no YAML
```

**An√°lise:**
- YAML define `max_output_tokens: 1500` mas n√£o √© aplicado
- `create_anthropic_client()` suporta `max_tokens` mas n√£o √© usado
- Respostas podem ser mais longas que necess√°rio para provoca√ß√µes socr√°ticas
- Filosofia socr√°tica: Provoca√ß√µes devem ser curtas e diretas (1-2 frases)

**Solu√ß√£o:**
```python
# agents/orchestrator/nodes.py
from agents.memory.config_loader import get_agent_context_limits

limits = get_agent_context_limits("orchestrator")
llm = create_anthropic_client(
    model=model_name, 
    temperature=0,
    max_tokens=limits["max_output_tokens"]  # ‚úÖ Aplicar limite do YAML
)
```

**Economia estimada:** 20-30% em tokens de output (5x impacto = 100-150% equivalente)

---

**Problema atual:**
```python
# agents/orchestrator/nodes.py:633, 640, 750
json.dumps(structurer_output, indent=2, ensure_ascii=False)
json.dumps(methodologist_output, indent=2, ensure_ascii=False)
json.dumps(previous_focal, indent=2, ensure_ascii=False)
```

**An√°lise:**
- `indent=2` adiciona ~30% de tokens (espa√ßos/linhas)
- Usado em 3+ locais no c√≥digo
- Para curadoria, formato compacto √© suficiente
- Indent s√≥ necess√°rio para logs/debugging

**Solu√ß√£o:**
```python
# Para contexto (compacto):
json.dumps(data, ensure_ascii=False)  # Sem indent

# Para logs (leg√≠vel):
json.dumps(data, indent=2, ensure_ascii=False)  # Com indent
```

**Economia estimada:** 5-10% em tokens de input (acumulado)

---

### 5. **Cache de Respostas Similares** üîÑ BAIXO IMPACTO (Futuro)

**Oportunidade:**
- Se usu√°rio faz pergunta similar a anterior, retornar resposta cached
- √ötil para perguntas frequentes sobre o sistema

**Complexidade:** Alta (requer sistema de cache + similaridade sem√¢ntica)  
**Prioridade:** Baixa (foco em otimiza√ß√µes mais simples primeiro)

---

## üìä Prioriza√ß√£o de Implementa√ß√£o

### Fase 1: Quick Wins (Alto Impacto, Baixa Complexidade)
1. **Truncar hist√≥rico de conversas no Orquestrador** (√∫ltimas 10 mensagens + resumo)
2. **Aplicar max_tokens nas respostas do Orquestrador** (usar limite do YAML)
3. **JSON compacto em contextos** (sem indent em 3 locais)

**Economia estimada:** 40-60% em conversas longas

### Fase 2: Otimiza√ß√µes de Prompt (M√©dio Impacto, M√©dia Complexidade)
4. **Otimizar prompt do Orquestrador** (reduzir de 615 para ~400 linhas)
   - Consolidar exemplos (7 ‚Üí 3)
   - Remover redund√¢ncias
   - Formato mais denso

**Economia estimada:** +20-30% adicional em tokens de input

### Fase 3: Otimiza√ß√µes Avan√ßadas (M√©dio Impacto, Alta Complexidade)
5. **Resumo incremental de hist√≥rico** (a cada 10 turnos)
6. **Cache de respostas** (futuro)

---

## üéØ Recomenda√ß√µes Espec√≠ficas

### 1. Truncamento de Hist√≥rico

```python
# agents/orchestrator/nodes.py
def _build_context(state: MultiAgentState, max_recent_messages: int = 10) -> str:
    messages = state.get("messages", [])
    
    if len(messages) > max_recent_messages:
        # √öltimas N mensagens completas
        recent = messages[-max_recent_messages:]
        # Resumo do restante
        old_summary = _summarize_old_messages(messages[:-max_recent_messages])
        context_parts.append(f"RESUMO DE CONVERSA ANTERIOR: {old_summary}")
        context_parts.append("HIST√ìRICO RECENTE:")
        # ... adicionar recent
    else:
        # ... c√≥digo atual
```

### 2. Aplicar max_tokens no Orquestrador

```python
# agents/orchestrator/nodes.py
from agents.memory.config_loader import get_agent_context_limits

# Carregar limites do YAML
limits = get_agent_context_limits("orchestrator")
max_output_tokens = limits.get("max_output_tokens", 1500)

# Aplicar na chamada
llm = create_anthropic_client(
    model=model_name, 
    temperature=0,
    max_tokens=max_output_tokens  # ‚úÖ Usar limite do YAML
)
```

### 3. JSON Compacto em Contextos

```python
# agents/orchestrator/nodes.py
# ‚ùå ANTES (indentado):
context_parts.append(json.dumps(structurer_output, indent=2, ensure_ascii=False))

# ‚úÖ DEPOIS (compacto):
context_parts.append(json.dumps(structurer_output, ensure_ascii=False))

# Para logs (manter indent):
logger.debug(f"Focal argument: {json.dumps(focal_argument, indent=2, ensure_ascii=False)}")
```

---

## üìà Proje√ß√£o de Economia

**Cen√°rio base atual (conversa de 20 turnos):**
- Input: ~15k tokens/turno √ó 20 = 300k tokens
  - Prompt base: ~15k tokens (ORCHESTRATOR_SOCRATIC_PROMPT_V1)
  - Hist√≥rico completo: ~10k tokens (todos os turnos)
  - JSON indentado: ~1k tokens
- Output: ~500 tokens/turno √ó 20 = 10k tokens (sem limite)
- **Custo (Haiku):** $0.24 + $0.04 = **$0.28**

**Cen√°rio otimizado (Fase 1 + 2):**
- Input: ~8k tokens/turno √ó 20 = 160k tokens
  - Prompt otimizado: ~10k tokens (redu√ß√£o de 33%)
  - Hist√≥rico truncado: ~3k tokens (√∫ltimas 10 + resumo)
  - JSON compacto: ~700 tokens (redu√ß√£o de 30%)
- Output: ~300 tokens/turno √ó 20 = 6k tokens (max_tokens aplicado)
- **Custo (Haiku):** $0.13 + $0.024 = **$0.154**

**Economia:** ~45% por conversa longa

---

## ‚úÖ Checklist de Implementa√ß√£o

### Cr√≠tico (Fase 1)
- [ ] **Truncar hist√≥rico no Orquestrador** (`agents/orchestrator/nodes.py:600-620`)
  - Implementar l√≥gica similar ao Observer (√∫ltimas 10 mensagens)
  - Adicionar resumo de mensagens antigas se > 10
- [ ] **Aplicar max_tokens no Orquestrador** (`agents/orchestrator/nodes.py:778`)
  - Carregar limite do YAML via `get_agent_context_limits("orchestrator")`
  - Passar `max_tokens` para `create_anthropic_client()`
- [ ] **JSON compacto em contextos** (`agents/orchestrator/nodes.py:633, 640, 750`)
  - Remover `indent=2` de contextos (manter apenas em logs)

### Importante (Fase 2)
- [ ] **Otimizar prompt do Orquestrador** (`utils/prompts/orchestrator.py`)
  - Reduzir exemplos de 7 para 3
  - Consolidar instru√ß√µes repetidas
  - Formato mais denso (menos linhas em branco)

### Monitoramento
- [ ] Monitorar m√©tricas de custo ap√≥s mudan√ßas
- [ ] Validar qualidade das respostas ap√≥s otimiza√ß√µes
- [ ] Documentar trade-offs (qualidade vs custo)

---

## üîó Refer√™ncias

- Vis√£o do produto: `products/revelar/docs/vision.md`
- Cost tracker: `utils/cost_tracker.py`
- Orquestrador: `agents/orchestrator/nodes.py`
- Configura√ß√µes: `config/agents/*.yaml`

