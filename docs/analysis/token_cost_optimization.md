# An√°lise: Otimiza√ß√£o de Custos de Tokens

**Data:** 2025-01-XX  
**Contexto:** An√°lise de oportunidades de economia de custos de tokens baseada na vis√£o do produto

---

## üéØ Vis√£o do Produto: Foco Estrat√©gico

**Princ√≠pio central:** "Lapidar UMA ideia por conversa" (n√£o assistente gen√©rico)

Esta filosofia permite otimiza√ß√µes agressivas que n√£o seriam poss√≠veis em sistemas generalistas:

- ‚úÖ **Foco estreito** = menos contexto necess√°rio
- ‚úÖ **Uma ideia por sess√£o** = hist√≥rico pode ser mais agressivamente truncado
- ‚úÖ **Dial√©tica socr√°tica** = respostas curtas e provocativas (n√£o explica√ß√µes longas)
- ‚úÖ **Agentes especializados** = cada um pode ter limites espec√≠ficos

---

## üí∞ Custos Atuais (USD por 1M tokens)

| Modelo | Input | Output | Ratio Output/Input |
|--------|-------|--------|-------------------|
| **Haiku** | $0.80 | $4.00 | 5x |
| **Sonnet** | $3.00 | $15.00 | 5x |
| **Opus** | $15.00 | $75.00 | 5x |

**Insight cr√≠tico:** Output √© 5x mais caro que input. Reduzir tokens de sa√≠da tem impacto 5x maior.

---

## üîç An√°lise de Oportunidades

### 1. **Hist√≥rico de Conversas Cresce Indefinidamente** ‚ö†Ô∏è CR√çTICO

**Problema atual:**
```python
# agents/orchestrator/nodes.py:600
messages = state.get("messages", [])
if messages:
    context_parts.append("HIST√ìRICO DA CONVERSA:")
    for msg in messages:  # ‚ùå TODAS as mensagens, sem limite
        context_parts.append(f"[Usu√°rio]: {msg.content}")
```

**Impacto:**
- Conversas longas enviam todo o hist√≥rico a cada chamada
- 20 turnos = ~10k tokens de hist√≥rico repetido
- Custo acumula exponencialmente

**Solu√ß√£o recomendada:**
1. **Truncamento inteligente:** √öltimas N mensagens + resumo do restante
2. **Resumo incremental:** A cada 10 turnos, resumir mensagens antigas
3. **Foco no argumento focal:** Usar `focal_argument` como contexto principal

**Economia estimada:** 30-50% em conversas longas (>10 turnos)

---

### 2. **Metodologista Usa Sonnet (5x Mais Caro)** ‚ö†Ô∏è ALTO IMPACTO

**Situa√ß√£o atual:**
```yaml
# config/agents/methodologist.yaml:69
model: claude-sonnet-4-20250514  # $3/$15 por 1M tokens
```

**An√°lise:**
- Metodologista valida hip√≥teses (tarefa estruturada)
- Haiku pode ser suficiente para valida√ß√£o estruturada
- Sonnet s√≥ necess√°rio se racioc√≠nio muito complexo

**Recomenda√ß√£o:**
1. **Testar Haiku primeiro:** Validar se qualidade √© suficiente
2. **Fallback para Sonnet:** Apenas se Haiku falhar consistentemente
3. **H√≠brido:** Haiku para valida√ß√£o simples, Sonnet para casos complexos

**Economia estimada:** 80% do custo do Metodologista (se migrar para Haiku)

---

### 3. **Prompts Podem Ser Mais Concisos** üìù M√âDIO IMPACTO

**Exemplo atual:**
```python
# utils/prompts/orchestrator.py:14
ORCHESTRATOR_SOCRATIC_PROMPT_V1 = """Voc√™ √© o Orquestrador Socr√°tico...
[~600 linhas de prompt]
"""
```

**Oportunidades:**
- Remover exemplos redundantes (manter apenas 1-2 melhores)
- Consolidar instru√ß√µes repetidas
- Usar formato mais denso (menos linhas em branco)

**Economia estimada:** 10-15% em tokens de input

---

### 4. **Limites de Cognitive Model J√° Existem** ‚úÖ BOM

**Implementa√ß√£o atual:**
```python
# agents/orchestrator/nodes.py:243-247
# Limites para evitar sobrecarga do prompt:
# - Proposi√ß√µes: 5 primeiras (ordenadas por solidez)
# - Conceitos: 10 primeiros
# - Contradi√ß√µes: 3 primeiras
# - Quest√µes abertas: 5 primeiras
```

**Status:** ‚úÖ J√° otimizado. Manter como est√°.

---

### 5. **Outputs de Agentes em JSON Completo** üìä M√âDIO IMPACTO

**Problema atual:**
```python
# agents/orchestrator/nodes.py:633
context_parts.append(json.dumps(structurer_output, indent=2, ensure_ascii=False))
```

**Oportunidade:**
- `indent=2` adiciona ~30% de tokens (espa√ßos/linhas)
- Para curadoria, formato compacto √© suficiente
- Manter indent apenas para logs/debugging

**Economia estimada:** 5-10% em tokens de input

---

### 6. **Respostas do Orquestrador Podem Ser Mais Curtas** üí¨ ALTO IMPACTO

**Filosofia socr√°tica:** Provoca√ß√µes devem ser curtas e diretas.

**Recomenda√ß√£o:**
- Adicionar `max_tokens` expl√≠cito nas chamadas
- Prompt: "Seja conciso. Provoca√ß√µes devem ter 1-2 frases."
- Limitar output a 300-500 tokens (suficiente para provoca√ß√£o)

**Economia estimada:** 20-30% em tokens de output (5x impacto = 100-150% equivalente)

---

### 7. **Cache de Respostas Similares** üîÑ BAIXO IMPACTO (Futuro)

**Oportunidade:**
- Se usu√°rio faz pergunta similar a anterior, retornar resposta cached
- √ötil para perguntas frequentes sobre o sistema

**Complexidade:** Alta (requer sistema de cache + similaridade sem√¢ntica)  
**Prioridade:** Baixa (foco em otimiza√ß√µes mais simples primeiro)

---

## üìä Prioriza√ß√£o de Implementa√ß√£o

### Fase 1: Quick Wins (Alto Impacto, Baixa Complexidade)
1. ‚úÖ **Truncar hist√≥rico de conversas** (√∫ltimas 10 mensagens + resumo)
2. ‚úÖ **Adicionar max_tokens nas respostas do Orquestrador** (300-500 tokens)
3. ‚úÖ **JSON compacto** (sem indent em contexto)

**Economia estimada:** 40-60% em conversas longas

### Fase 2: Testes de Modelo (Alto Impacto, M√©dia Complexidade)
4. ‚úÖ **Testar Haiku no Metodologista** (validar qualidade)
5. ‚úÖ **Otimizar prompts** (remover redund√¢ncias)

**Economia estimada:** +20-30% adicional

### Fase 3: Otimiza√ß√µes Avan√ßadas (M√©dio Impacto, Alta Complexidade)
6. ‚è≥ **Resumo incremental de hist√≥rico** (a cada 10 turnos)
7. ‚è≥ **Cache de respostas** (futuro)

---

## üéØ Recomenda√ß√µes Espec√≠ficas

### 1. Truncamento de Hist√≥rico

```python
# agents/orchestrator/nodes.py
def _build_context(state: MultiAgentState, max_recent_messages: int = 10) -> str:
    messages = state.get("messages", [])
    
    if len(messages) > max_recent_messages:
        # √öltimas N mensagens completas
        recent = messages[-max_recent_messages:]
        # Resumo do restante
        old_summary = _summarize_old_messages(messages[:-max_recent_messages])
        context_parts.append(f"RESUMO DE CONVERSA ANTERIOR: {old_summary}")
        context_parts.append("HIST√ìRICO RECENTE:")
        # ... adicionar recent
    else:
        # ... c√≥digo atual
```

### 2. Limitar Output do Orquestrador

```python
# agents/orchestrator/nodes.py
response = llm.invoke(
    messages,
    max_tokens=400  # Provoca√ß√µes curtas (socr√°tico)
)
```

### 3. Testar Haiku no Metodologista

```yaml
# config/agents/methodologist.yaml
model: claude-3-5-haiku-20241022  # Testar primeiro
# Fallback para Sonnet apenas se necess√°rio
```

---

## üìà Proje√ß√£o de Economia

**Cen√°rio base (conversa de 20 turnos):**
- Input: ~15k tokens/turno √ó 20 = 300k tokens
- Output: ~500 tokens/turno √ó 20 = 10k tokens
- **Custo (Haiku):** $0.24 + $0.04 = **$0.28**

**Cen√°rio otimizado (Fase 1 + 2):**
- Input: ~8k tokens/turno √ó 20 = 160k tokens (truncamento + JSON compacto)
- Output: ~300 tokens/turno √ó 20 = 6k tokens (max_tokens)
- **Custo (Haiku):** $0.13 + $0.024 = **$0.154**

**Economia:** ~45% por conversa longa

**Se Metodologista migrar para Haiku:**
- Economia adicional: ~80% do custo do Metodologista
- **Total:** ~60-70% de economia em sess√µes completas

---

## ‚úÖ Checklist de Implementa√ß√£o

- [ ] Implementar truncamento de hist√≥rico (√∫ltimas 10 + resumo)
- [ ] Adicionar max_tokens=500 nas respostas do Orquestrador
- [ ] JSON compacto em contextos (sem indent)
- [ ] Migrar Metodologista para Haiku
- [ ] Otimizar prompts (remover redund√¢ncias)
- [ ] Monitorar m√©tricas de custo ap√≥s mudan√ßas
- [ ] Documentar trade-offs (qualidade vs custo)

---

## üîó Refer√™ncias

- Vis√£o do produto: `docs/vision/vision.md`
- Cost tracker: `utils/cost_tracker.py`
- Orquestrador: `agents/orchestrator/nodes.py`
- Configura√ß√µes: `config/agents/*.yaml`

