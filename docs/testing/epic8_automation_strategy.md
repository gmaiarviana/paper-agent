# √âPICO 8: An√°lise Assistida de Qualidade - Ferramentas para Discuss√£o

> **Objetivo:** Facilitar an√°lise humana de qualidade conversacional atrav√©s de ferramentas que estruturam dados para discuss√£o eficiente com LLM.

---

## üìã Vis√£o Geral

**Depend√™ncia:** √âpico 7 deve estar conclu√≠do

**Insight do √âpico 7:**
O valor N√ÉO veio de automa√ß√£o, mas de **discuss√£o contextualizada**:
- Investiga√ß√£o interativa ("me mostre os logs", "por que isso?")
- Decis√µes estrat√©gicas debatidas (baseline opcional?)
- Planejamento adaptativo (pivotamos de manual ‚Üí automatizado)
- **Humano + Claude analisando JUNTOS**

**Problema:**
- Valida√ß√£o manual (√âpico 7) foi eficaz mas trabalhosa
- Precisamos reduzir tempo de setup
- MAS: Automa√ß√£o completa perde contexto e qualidade

**Solu√ß√£o:**
- Ferramentas que **estruturam dados** para an√°lise
- Output formatado para **f√°cil discuss√£o** com LLM
- Humano + Claude fazem an√°lise (n√£o script)
- **Mant√©m qualidade, reduz trabalho manual**

**Resultado Esperado:**
- Rodar cen√°rio completo: 1 comando
- Gerar relat√≥rio estruturado: autom√°tico
- Colar no Claude e discutir: 30 segundos
- Identificar causa raiz: minutos (n√£o horas)
- **Replic√°vel:** Pr√≥xima vez √© mais r√°pido

---

## üìö Aprendizados do √âpico 7 que Moldaram este √âpico

### 1. Multi-Turn √â Cr√≠tico, N√£o Opcional

**Problema identificado:**
- Cen√°rios 3 e 6 (√âpico 7) n√£o foram validados completamente
- Script single-turn s√≥ testa primeiro turno
- Fluxos Orchestrator ‚Üí Structurer ‚Üí Methodologist n√£o foram exercitados

**Impacto no √âpico 8:**
- Multi-turn executor deve ser funcionalidade PRIORIT√ÅRIA
- LLM-as-Judge sozinho n√£o resolve (precisa de conversas completas)
- Framework deve suportar valida√ß√£o de fluxos end-to-end

### 2. Debug de Logs Foi Cr√≠tico para Troubleshooting

**Problema identificado:**
- Sem logs detalhados (`debug_scenario_2.py`), n√£o achamos causa raiz
- Reasoning completo do LLM revelou decis√µes sutis
- Prompt bloqueava comportamento ("Turno 1: sempre explore")

**Impacto no √âpico 8:**
- Debug mode deve estar embutido no framework
- Compara√ß√£o antes/depois de mudan√ßas no prompt
- Logs devem ser salvos automaticamente

### 3. Problemas S√£o Sutis e Dif√≠ceis de Detectar

**Problema identificado:**
- "Observei que..." vs claim direto muda intent
- "Posso chamar X?" vs "Vou validar X" muda fluidez
- Baseline opcional vs obrigat√≥rio afeta transi√ß√µes

**Impacto no √âpico 8:**
- LLM-as-Judge deve avaliar NUANCES (n√£o s√≥ pass/fail)
- Crit√©rios devem cobrir antipadr√µes espec√≠ficos
- Valida√ß√£o de qualidade conversacional (n√£o apenas estrutura)

### 4. Regress√£o Pode Acontecer Facilmente

**Problema identificado:**
- Corrigir Turno 1 poderia quebrar outros cen√°rios
- Mudan√ßas no prompt t√™m efeitos colaterais
- Precisamos baseline para comparar

**Impacto no √âpico 8:**
- Regression detector (comparar antes/depois)
- Alertas de regress√£o (score cai, custo aumenta)
- Baseline de qualidade preservado

### 5. CI/CD √â Prematuro Nesta Fase

**Decis√£o:**
- N√£o temos reposit√≥rio p√∫blico nem m√∫ltiplos desenvolvedores
- N√£o temos deploy cont√≠nuo
- Framework local √© suficiente por enquanto

**Impacto no √âpico 8:**
- CI/CD removido do escopo (postergar para √âpico 10+)
- Foco em execu√ß√£o local e valida√ß√£o manual assistida

---

## üéØ Filosofia: Assistir, N√£o Substituir

### O Que N√ÉO Fazer ‚ùå

**Automa√ß√£o prematura:**
```python
# Script roda testes
score = llm_judge.evaluate(result)  # "4/5 - Boa fluidez"
print(f"Score: {score}")  # E da√≠? O que fazer com isso?
```

**Problema:** Voc√™ perde contexto, nuances, capacidade de adapta√ß√£o.

### O Que Fazer ‚úÖ

**An√°lise assistida:**
```bash
# 1. Rodar cen√°rio
python scripts/testing/run_scenario.py --scenario 2

# 2. Output estruturado
========================================
CEN√ÅRIO 2: An√°lise Necess√°ria
========================================
Input: "Claude Code reduz tempo..."
Esperado: suggest_agent
Observado: explore ‚ùå

Logs:
  orchestrator_analysis: "Turno 1, sempre explore..."
  next_step: explore (PROBLEMA)

Pergunta sugerida:
Por que sistema n√£o reconheceu contexto suficiente?
========================================

# 3. Voc√™ copia e cola no Claude
# 4. Claude analisa e sugere causa raiz
# 5. Voc√™ decide pr√≥ximos passos
```

**Benef√≠cio:** Mant√©m qualidade da an√°lise + reduz setup manual.

---

## üéØ O Que Automatizar

**Princ√≠pio:** Automatizar valida√ß√£o de **problemas reais identificados no √âpico 7** atrav√©s de conversas completas end-to-end.

**Prioridade 1: Multi-Turn Validation** üî¥
- Cen√°rios 3 e 6 requerem m√∫ltiplos turnos (n√£o foram completamente validados)
- Fluxos Orchestrator ‚Üí Agent ‚Üí Orchestrator (curadoria)
- Preserva√ß√£o de contexto ao longo de 3-5 turnos
- Transi√ß√µes autom√°ticas (sem pedir permiss√£o)

**Prioridade 2: LLM-as-Judge para Qualidade** üî¥
- Fluidez conversacional (n√£o pede permiss√£o)
- Comportamento socr√°tico (provoca√ß√£o genu√≠na)
- Curadoria (n√£o dump t√©cnico)
- Decis√µes coerentes (crit√©rios expl√≠citos)

**Prioridade 3: Debug & Regression** üü°
- Debug mode (logs detalhados para troubleshooting)
- Regression detector (detectar quebras ap√≥s mudan√ßas)
- Compara√ß√£o antes/depois de altera√ß√µes no prompt

**N√ÉO automatizar neste √©pico:**
- ‚ùå CI/CD (prematuro - postergar para √âpico 10+)
- ‚ùå Problemas hipot√©ticos n√£o encontrados no √âpico 7
- ‚ùå Valida√ß√£o de estrutura (testes unit√°rios j√° fazem isso)
- ‚ùå Testes determin√≠sticos (usar testes de integra√ß√£o normais)

---

## üîÑ Multi-Turn Executor (PRIORIDADE #1)

### Objetivo
Executar conversas completas end-to-end para validar fluxos multi-agente que n√£o foram testados no √âpico 7.

### Motiva√ß√£o (√âpico 7)
- **Cen√°rio 3:** User vago ‚Üí Orchestrator ‚Üí Structurer ‚Üí Methodologist ‚Üí needs_refinement
- **Cen√°rio 6:** User vago ‚Üí Methodologist ‚Üí pede clarifica√ß√£o ‚Üí User responde ‚Üí Methodologist valida
- **Cen√°rio 7:** 5 turnos com evolu√ß√£o de focal_argument

Script single-turn s√≥ validou primeiro turno. Fluxos completos n√£o foram exercitados.

### Componentes

#### 1. `ConversationScenario` (Data Class)

**Localiza√ß√£o:** `utils/test_scenarios.py`

```python
from dataclasses import dataclass
from typing import List, Tuple, Optional

@dataclass
class ConversationScenario:
    """Define cen√°rio de conversa multi-turn."""
    
    id: str
    description: str
    turns: List[Tuple[str, str]]  # [("user", "input"), ("system", "expected_action"), ...]
    expected_agents: List[str]    # Agentes que devem ser chamados
    expected_final_state: dict    # Estado esperado ao final
    
    @classmethod
    def from_epic7_scenario(cls, scenario_number: int) -> "ConversationScenario":
        """Cria cen√°rio baseado em cen√°rios do √âpico 7."""
        scenarios = {
            3: cls(
                id="cenario_03_refinamento",
                description="Ideia vaga evolui para Structurer ‚Üí Methodologist ‚Üí needs_refinement",
                turns=[
                    ("user", "M√©todo X melhora desenvolvimento"),
                    ("system", "explore"),  # Orchestrator pergunta sobre m√©trica
                    ("user", "Melhora velocidade de entrega"),
                    ("system", "suggest_agent"),  # Chama Structurer
                    ("structurer", "structured_question"),
                    ("system", "suggest_agent"),  # Chama Methodologist
                    ("methodologist", "needs_refinement"),
                    ("system", "curadoria")  # Apresenta feedback
                ],
                expected_agents=["orchestrator", "structurer", "methodologist"],
                expected_final_state={
                    "next_step": "explore",
                    "methodologist_output": {"status": "needs_refinement"}
                }
            ),
            # ... outros cen√°rios
        }
        return scenarios[scenario_number]
```

#### 2. `MultiTurnExecutor` (Executor)

**Localiza√ß√£o:** `utils/test_executor.py`

```python
from typing import List, Dict
from langchain_core.messages import HumanMessage, AIMessage

class MultiTurnExecutor:
    """Executa cen√°rios com m√∫ltiplos turnos."""
    
    def __init__(self, graph):
        self.graph = graph
        self.conversation_history = []
    
    def execute_scenario(self, scenario: ConversationScenario) -> Dict:
        """
        Executa cen√°rio completo turn-by-turn.
        
        Returns:
            {
                "turns": [resultado de cada turno],
                "final_state": estado final,
                "agents_called": lista de agentes acionados,
                "metrics": {tokens, custo, dura√ß√£o},
                "success": bool
            }
        """
        state = create_initial_multi_agent_state(
            scenario.turns[0][1],  # Primeiro input do usu√°rio
            session_id=f"test-{scenario.id}"
        )
        
        results = []
        agents_called = []
        
        for turn_type, content in scenario.turns:
            if turn_type == "user":
                # Adicionar input do usu√°rio ao estado
                state["messages"].append(HumanMessage(content=content))
                
                # Executar grafo
                result = self.graph.invoke(state)
                
                # Rastrear agentes chamados
                if result.get("next_step") == "suggest_agent":
                    agent = result.get("agent_suggestion", {}).get("agent")
                    if agent:
                        agents_called.append(agent)
                
                # Salvar resultado do turno
                results.append({
                    "turn": len(results) + 1,
                    "user_input": content,
                    "system_response": result.get("messages", [])[-1].content if result.get("messages") else None,
                    "next_step": result.get("next_step"),
                    "focal_argument": result.get("focal_argument")
                })
                
                # Atualizar estado para pr√≥ximo turno
                state = result
        
        return {
            "turns": results,
            "final_state": state,
            "agents_called": agents_called,
            "success": self._validate_scenario(scenario, agents_called, state)
        }
    
    def _validate_scenario(self, scenario: ConversationScenario, agents_called: List[str], final_state: Dict) -> bool:
        """Valida que cen√°rio executou conforme esperado."""
        # Verificar que agentes esperados foram chamados
        if set(scenario.expected_agents) != set(agents_called):
            return False
        
        # Verificar estado final
        for key, expected_value in scenario.expected_final_state.items():
            if final_state.get(key) != expected_value:
                return False
        
        return True
    
    def generate_analysis_report(self, result: Dict) -> str:
        """
        Gera relat√≥rio formatado para an√°lise humana.
        
        Output estruturado para f√°cil c√≥pia e discuss√£o com LLM:
        - Contexto do cen√°rio
        - Comportamento esperado vs observado
        - Logs relevantes
        - Problemas detectados automaticamente
        - Perguntas sugeridas para Claude
        """
        report = []
        report.append("=" * 60)
        report.append(f"CEN√ÅRIO: {result.get('scenario_id', 'Unknown')}")
        report.append("=" * 60)
        report.append("")
        report.append("## Contexto")
        report.append(f"Input inicial: {result.get('turns', [{}])[0].get('user_input', 'N/A')}")
        report.append(f"Agentes esperados: {', '.join(result.get('expected_agents', []))}")
        report.append(f"Agentes chamados: {', '.join(result.get('agents_called', []))}")
        report.append("")
        report.append("## Resultado")
        if result.get("success"):
            report.append("‚úÖ Cen√°rio executou conforme esperado")
        else:
            report.append("‚ùå Problemas detectados:")
            # Detectar problemas espec√≠ficos
            if set(result.get('expected_agents', [])) != set(result.get('agents_called', [])):
                report.append(f"  - Agentes esperados n√£o foram chamados")
            report.append("")
            report.append("## Logs Relevantes")
            for turn in result.get("turns", []):
                report.append(f"[Turn {turn.get('turn')}]")
                report.append(f"  Input: {turn.get('user_input')}")
                report.append(f"  next_step: {turn.get('next_step')}")
                if turn.get('focal_argument'):
                    report.append(f"  focal_argument: {turn.get('focal_argument')}")
                report.append("")
        report.append("=" * 60)
        report.append("Copie acima e pergunte ao Claude:")
        report.append('"Por que o sistema n√£o executou conforme esperado?"')
        report.append("=" * 60)
        return "\n".join(report)
```

#### 3. Fixture `multi_turn_executor`

**Localiza√ß√£o:** `tests/conftest.py`

```python
@pytest.fixture
def multi_turn_executor(multi_agent_graph):
    """Fixture para executor multi-turn."""
    from utils.test_executor import MultiTurnExecutor
    return MultiTurnExecutor(multi_agent_graph)
```

### Uso nos Testes

**Exemplo em `tests/integration/test_multi_turn_flows.py`:**

```python
@pytest.mark.integration
def test_cenario_3_refinement_flow(multi_turn_executor):
    """Valida fluxo completo do Cen√°rio 3 (refinamento)."""
    from utils.test_scenarios import ConversationScenario
    
    scenario = ConversationScenario.from_epic7_scenario(3)
    result = multi_turn_executor.execute_scenario(scenario)
    
    # Valida√ß√µes estruturais
    assert result["success"], "Cen√°rio n√£o executou conforme esperado"
    assert "structurer" in result["agents_called"]
    assert "methodologist" in result["agents_called"]
    assert result["final_state"]["methodologist_output"]["status"] == "needs_refinement"

@pytest.mark.llm_judge
def test_cenario_3_quality(multi_turn_executor, llm_judge):
    """Valida qualidade conversacional do Cen√°rio 3."""
    scenario = ConversationScenario.from_epic7_scenario(3)
    result = multi_turn_executor.execute_scenario(scenario)
    
    # Extrair mensagens ao usu√°rio
    messages = [turn["system_response"] for turn in result["turns"] if turn["system_response"]]
    
    # Avaliar fluidez de cada mensagem
    for message in messages:
        evaluation = llm_judge.invoke(FLUENCY_PROMPT.format(message=message))
        score = extract_score(evaluation.content)
        assert score >= 4, f"Mensagem n√£o fluida: {message[:50]}... (score: {score})"
```

### Uso via Script

```bash
python scripts/testing/run_scenario.py --scenario 3

# Output:
========================================
CEN√ÅRIO 3: Refinamento Multi-Agente
========================================

## Contexto
Usu√°rio: "M√©todo X melhora desenvolvimento"
Esperado: Orchestrator ‚Üí Structurer ‚Üí Methodologist

## Resultado
‚úÖ Orchestrator explorou m√©trica
‚úÖ Structurer foi chamado
‚ùå Methodologist N√ÉO foi chamado (esperado)

## Logs Relevantes
[Turn 3 - ap√≥s Structurer]
  next_step: explore (esperado: suggest_agent)
  orchestrator_analysis: "Aguardando mais contexto..."

## Problema Detectado
Sistema n√£o chamou Methodologist ap√≥s Structurer.
Poss√≠vel causa: Crit√©rio de transi√ß√£o muito conservador.

## Sugest√£o de An√°lise
Copie este relat√≥rio e pergunte ao Claude:
"Por que o sistema n√£o chamou Methodologist ap√≥s 
o Structurer criar a quest√£o estruturada?"
========================================
```

---

## üêõ Debug Mode (PRIORIDADE #2)

### Objetivo
Facilitar troubleshooting de problemas sutis atrav√©s de logs detalhados.

### Motiva√ß√£o (√âpico 7)
- Debug script (`debug_scenario_2.py`) foi CR√çTICO para encontrar causa raiz
- Logs revelaram reasoning completo do LLM
- Mostrou onde decis√£o foi tomada ("Turno 1: sempre explore")

### Componentes

#### 1. `DebugExecutor`

**Localiza√ß√£o:** `utils/test_executor.py` (adicionar ao arquivo existente)

```python
from pathlib import Path
from datetime import datetime
import json

class DebugExecutor(MultiTurnExecutor):
    """Executor com logging detalhado para troubleshooting."""
    
    def __init__(self, graph, debug_dir: str = "logs/debug"):
        super().__init__(graph)
        self.debug_dir = Path(debug_dir)
        self.debug_dir.mkdir(parents=True, exist_ok=True)
    
    def execute_with_debug(self, scenario: ConversationScenario) -> Dict:
        """
        Executa cen√°rio com logging completo.
        
        Salva em arquivo:
        - Prompt completo enviado ao LLM
        - Resposta bruta antes de parsing
        - Reasoning do LLM
        - Decis√µes tomadas (next_step, agent_suggestion)
        - Estado antes/depois de cada turno
        """
        debug_log = {
            "scenario_id": scenario.id,
            "timestamp": datetime.now().isoformat(),
            "turns": []
        }
        
        # Executar com logging
        state = create_initial_multi_agent_state(
            scenario.turns[0][1],
            session_id=f"debug-{scenario.id}"
        )
        
        for turn_type, content in scenario.turns:
            if turn_type == "user":
                # Salvar estado antes
                turn_log = {
                    "turn": len(debug_log["turns"]) + 1,
                    "user_input": content,
                    "state_before": self._serialize_state(state),
                }
                
                # Executar
                state["messages"].append(HumanMessage(content=content))
                result = self.graph.invoke(state)
                
                # Salvar estado depois e reasoning
                turn_log["state_after"] = self._serialize_state(result)
                turn_log["llm_reasoning"] = result.get("orchestrator_analysis", "")
                turn_log["next_step"] = result.get("next_step")
                turn_log["agent_suggestion"] = result.get("agent_suggestion")
                
                debug_log["turns"].append(turn_log)
                state = result
        
        # Salvar log em arquivo
        log_file = self.debug_dir / f"{scenario.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(debug_log, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Debug log salvo: {log_file}")
        
        return {
            "scenario": scenario,
            "result": result,
            "log_file": str(log_file)
        }
    
    def _serialize_state(self, state: Dict) -> Dict:
        """Serializa estado para logging (remove objetos n√£o serializ√°veis)."""
        return {
            "next_step": state.get("next_step"),
            "focal_argument": state.get("focal_argument"),
            "messages_count": len(state.get("messages", [])),
            "last_message": str(state.get("messages", [])[-1]) if state.get("messages") else None
        }
    
    def generate_debug_report(self, debug_result: Dict) -> str:
        """
        Gera relat√≥rio de debug formatado para an√°lise.
        
        Inclui:
        - Prompt completo enviado ao LLM
        - Resposta bruta antes de parsing
        - Reasoning do LLM
        - Decis√µes tomadas step-by-step
        - Estado antes/depois de cada transi√ß√£o
        """
        report = []
        report.append("=" * 60)
        report.append(f"DEBUG REPORT: {debug_result['scenario'].id}")
        report.append("=" * 60)
        
        with open(debug_result["log_file"], "r", encoding="utf-8") as f:
            debug_log = json.load(f)
        
        for turn in debug_log["turns"]:
            report.append(f"\n[TURN {turn['turn']}]")
            report.append(f"Input: {turn['user_input']}")
            report.append(f"\n[LLM REASONING]")
            report.append(turn.get('llm_reasoning', 'N/A'))
            report.append(f"\n[DECISION]")
            report.append(f"next_step: {turn.get('next_step')}")
            if turn.get('agent_suggestion'):
                report.append(f"agent: {turn['agent_suggestion'].get('agent')}")
            report.append(f"\n{'-' * 60}")
        
        report.append("\n" + "=" * 60)
        report.append("Copie acima e pergunte ao Claude:")
        report.append('"Onde o reasoning levou √† decis√£o errada?"')
        report.append("=" * 60)
        
        return "\n".join(report)
    
    def compare_prompts(self, scenario: ConversationScenario, old_prompt: str, new_prompt: str) -> Dict:
        """
        Compara comportamento antes/depois de mudan√ßa no prompt.
        
        Returns:
            {
                "scenario_id": str,
                "changes_detected": bool,
                "before": {resultado com prompt antigo},
                "after": {resultado com prompt novo},
                "diff": {diferen√ßas encontradas}
            }
        """
        # TODO: Implementar compara√ß√£o de prompts
        # Requer salvar baseline antes de mudan√ßa
        pass
```

#### 2. Fixture `debug_executor`

**Localiza√ß√£o:** `tests/conftest.py`

```python
@pytest.fixture
def debug_executor(multi_agent_graph):
    """Fixture para executor com debug mode."""
    from utils.test_executor import DebugExecutor
    return DebugExecutor(multi_agent_graph)
```

### Uso para Troubleshooting

```bash
# Via script
python scripts/testing/debug_scenario.py --scenario 2

# Gera debug report completo
# Mostra reasoning detalhado do LLM
# Pronto para colar no Claude e investigar
```

```python
# Em qualquer teste, adicionar --debug
pytest tests/integration/test_cenario_2.py --debug

# Ou programaticamente
def test_debug_cenario_2(debug_executor):
    """Debug do Cen√°rio 2 (hip√≥tese completa)."""
    from utils.test_scenarios import ConversationScenario
    
    scenario = ConversationScenario.from_epic7_scenario(2)
    result = debug_executor.execute_with_debug(scenario)
    
    # Log ser√° salvo em logs/debug/cenario_02_*.json
    # Pode ser analisado manualmente
    
    report = debug_executor.generate_debug_report(result)
    print(report)
```

---

## üõ†Ô∏è Infraestrutura LLM-as-Judge (PRIORIDADE #3)

**Nota:** LLM-as-Judge √© usado EM CONJUNTO com Multi-Turn Executor para validar qualidade conversacional ap√≥s execu√ß√£o completa do cen√°rio.

### 1. Fixture `llm_judge`

**Localiza√ß√£o:** `tests/conftest.py`

**Especifica√ß√£o:**
```python
@pytest.fixture
def llm_judge():
    """
    Fixture para LLM-as-judge (avaliador de qualidade).
    
    Usa Claude Haiku para custo-benef√≠cio.
    Temperature=0 para determinismo.
    """
    import os
    from langchain_anthropic import ChatAnthropic
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        pytest.skip("LLM-as-judge test skipped: ANTHROPIC_API_KEY not set")
    
    return ChatAnthropic(
        model="claude-3-5-haiku-20241022",
        temperature=0
    )
```

**Caracter√≠sticas:**
- Usa Haiku (custo-benef√≠cio)
- Temperature=0 (determin√≠stico)
- Pula testes se API key n√£o est√° definida (n√£o falha)

---

### 2. Prompts de Avalia√ß√£o

**Localiza√ß√£o:** `utils/test_prompts.py`

**5 Prompts Necess√°rios:**

#### 2.1 Fluidez Conversacional
```python
FLUENCY_PROMPT = """
Avalie a fluidez da mensagem do sistema:

1. N√£o pergunta permiss√£o ("Posso chamar X?")
2. Integra√ß√£o natural de outputs de agentes
3. Tom conversacional (n√£o burocr√°tico)

Mensagem: {message}

Avalie de 1-5 (5 = completamente fluida):
Justificativa:
"""
```

#### 2.2 Integra√ß√£o Entre Agentes
```python
INTEGRATION_QUALITY_PROMPT = """
Avalie a qualidade da integra√ß√£o entre agentes:

1. Transi√ß√µes naturais (sem quebras)
2. Contexto preservado (refer√™ncias a turnos anteriores)
3. Experi√™ncia coesa (n√£o parece sistema desconexo)

Orquestrador: {orchestrator_output}
Estruturador: {structurer_output}
Metodologista: {methodologist_output}
Mensagens ao usu√°rio: {messages}

Avalie de 1-5 (5 = integra√ß√£o excelente):
Justificativa:
"""
```

#### 2.3 Provoca√ß√£o Socr√°tica
```python
SOCRATIC_BEHAVIOR_PROMPT = """
Avalie se a resposta do sistema demonstra comportamento socr√°tico genu√≠no:

1. Provoca√ß√£o genu√≠na (exp√µe assumptions, n√£o coleta burocr√°tica)
2. Timing natural (n√£o regras fixas)
3. Parada inteligente (n√£o insiste infinitamente)

Resposta: {response}
Reflection prompt: {reflection_prompt}

Avalie de 1-5 (5 = excelente comportamento socr√°tico):
Justificativa:
"""
```

#### 2.4 Preserva√ß√£o de Contexto
```python
CONTEXT_PRESERVATION_PROMPT = """
Avalie se o contexto foi preservado entre transi√ß√µes de agentes:

1. Focal argument evolui coerentemente
2. Informa√ß√µes de turnos anteriores s√£o referenciadas
3. N√£o h√° perda de contexto (agente n√£o "esquece" informa√ß√µes)

Focal argument (antes): {focal_before}
Focal argument (depois): {focal_after}
Mensagens: {messages}

Avalie de 1-5 (5 = contexto perfeitamente preservado):
Justificativa:
"""
```

#### 2.5 Qualidade de Decis√µes
```python
DECISION_QUALITY_PROMPT = """
Avalie a qualidade da decis√£o do agente:

1. Decis√£o √© coerente com contexto fornecido
2. Justificativa √© clara e espec√≠fica
3. N√£o √© arbitr√°ria (usa crit√©rios expl√≠citos)

Contexto: {context}
Decis√£o: {decision}
Justificativa: {justification}

Avalie de 1-5 (5 = decis√£o excelente):
Justificativa:
"""
```

---

### 3. Helper `extract_score`

**Localiza√ß√£o:** `utils/test_helpers.py`

**Especifica√ß√£o:**
```python
import re

def extract_score(evaluation_content: str) -> int:
    """
    Extrai score (1-5) da avalia√ß√£o do LLM-as-judge.
    
    Procura por padr√µes:
    - "Avalie de 1-5: 4"
    - "score: 3"
    - "4/5"
    - Apenas n√∫mero na linha
    
    Args:
        evaluation_content: Conte√∫do da avalia√ß√£o do LLM
        
    Returns:
        int: Score de 1-5
        
    Raises:
        ValueError: Se n√£o encontrar score v√°lido
    """
    patterns = [
        r"Avalie de 1-5.*?(\d)",
        r"score.*?(\d)",
        r"(\d)\s*/\s*5",
        r"(\d)\s*=\s*(?:excelente|√≥timo|bom)",
        r"^(\d)$"  # Apenas n√∫mero na linha
    ]
    
    for pattern in patterns:
        match = re.search(pattern, evaluation_content, re.IGNORECASE | re.MULTILINE)
        if match:
            score = int(match.group(1))
            if 1 <= score <= 5:
                return score
    
    raise ValueError(f"N√£o foi poss√≠vel extrair score v√°lido de: {evaluation_content}")
```

---

### 4. Marker no `pytest.ini`

**Adicionar:**
```ini
[pytest]
markers =
    unit: Testes unit√°rios (mocks)
    integration: Testes de integra√ß√£o (API real)
    llm_judge: Testes que usam LLM-as-judge (requer API key)
    slow: Testes lentos (opcional)
```

---

## üìù Testes Automatizados

### Princ√≠pio: Adicionar Valida√ß√£o de Qualidade

**N√ÉO substituir testes existentes**  
**ADICIONAR** fun√ß√£o de teste com `@pytest.mark.llm_judge`

**Exemplo:**
```python
# Teste existente (estrutura)
def test_multi_agent_flow(multi_agent_graph):
    result = multi_agent_graph.invoke(state)
    assert result["orchestrator_analysis"] is not None
    assert result["next_step"] in ["explore", "suggest_agent"]

# ADICIONAR: Teste de qualidade
@pytest.mark.llm_judge
def test_multi_agent_flow_quality(multi_agent_graph, llm_judge):
    """Valida qualidade da experi√™ncia conversacional."""
    result = multi_agent_graph.invoke(state)
    
    # Valida√ß√£o estrutural (mant√©m)
    assert result["orchestrator_analysis"] is not None
    
    # NOVO: Valida√ß√£o de qualidade
    evaluation = llm_judge.invoke(
        CONVERSATION_QUALITY_PROMPT.format(
            response=result.get("messages", [])[-1].content,
            history=result.get("conversation_history", [])
        )
    )
    score = extract_score(evaluation.content)
    assert score >= 4, f"Qualidade conversacional insuficiente (score: {score})"
```

---

### Arquivos a Adicionar Testes

Baseado no **√âpico 7** (problemas identificados), adicionar testes em:

#### 1. `tests/integration/test_multi_agent_smoke.py`
**Validar:**
- Fluidez conversacional (sem "Posso chamar X?")
- Integra√ß√£o entre agentes (transi√ß√µes naturais)
- Preserva√ß√£o de contexto (focal_argument evolui)

**Exemplo:**
```python
@pytest.mark.llm_judge
def test_conversational_fluency(multi_agent_graph, llm_judge):
    """Valida que sistema n√£o pede permiss√£o para transi√ß√µes."""
    state = create_initial_multi_agent_state(
        "Observei que LLMs aumentam produtividade",
        session_id="test-fluency-1"
    )
    
    result = multi_agent_graph.invoke(state)
    
    # Extrair mensagens ao usu√°rio
    user_messages = [
        msg.content for msg in result.get("messages", [])
        if isinstance(msg, AIMessage)
    ]
    
    # Validar cada mensagem
    for message in user_messages:
        evaluation = llm_judge.invoke(
            FLUENCY_PROMPT.format(message=message)
        )
        score = extract_score(evaluation.content)
        assert score >= 4, f"Mensagem n√£o √© fluida: {message[:50]}... (score: {score})"
```

---

#### 2. `tests/integration/test_methodologist_smoke.py`
**Validar:**
- Perguntas s√£o socr√°ticas (n√£o burocr√°ticas)
- Decis√µes t√™m crit√©rios claros (n√£o arbitr√°rias)

**Exemplo:**
```python
@pytest.mark.llm_judge
def test_socratic_questions_quality(methodologist_graph, llm_judge):
    """Valida que perguntas do Metodologista s√£o socr√°ticas."""
    state = create_initial_methodologist_state(
        "Caf√© aumenta produtividade"
    )
    
    result = methodologist_graph.invoke(state)
    
    if result.get("status") == "pending":
        clarifications = result.get("clarifications", {})
        
        for question in clarifications.keys():
            evaluation = llm_judge.invoke(
                SOCRATIC_QUESTION_PROMPT.format(question=question)
            )
            score = extract_score(evaluation.content)
            assert score >= 4, f"Pergunta n√£o √© socr√°tica: {question} (score: {score})"
```

---

#### 3. `scripts/flows/validate_socratic_behavior.py` ‚Üí Converter para teste automatizado
**Validar:**
- Provoca√ß√£o socr√°tica genu√≠na (exp√µe assumptions)
- Timing natural (n√£o regras fixas)
- Parada inteligente (n√£o insiste infinitamente)

**Exemplo:**
```python
@pytest.mark.llm_judge
def test_socratic_provocation_quality(orchestrator_node, llm_judge):
    """Valida que provoca√ß√£o socr√°tica √© genu√≠na."""
    state = create_state_with_vague_metric(
        "Quero medir produtividade"
    )
    
    result = orchestrator_node(state)
    
    reflection_prompt = result.get("reflection_prompt", "")
    response = result.get("messages", [])[-1].content
    
    evaluation = llm_judge.invoke(
        SOCRATIC_BEHAVIOR_PROMPT.format(
            response=response,
            reflection_prompt=reflection_prompt
        )
    )
    score = extract_score(evaluation.content)
    assert score >= 4, f"Provoca√ß√£o n√£o √© socr√°tica (score: {score})"
```

---

#### 4. `scripts/flows/validate_conversation_flow.py` ‚Üí Converter para teste automatizado
**Validar:**
- Fluidez conversacional end-to-end
- N√£o h√° quebras entre transi√ß√µes

---

#### 5. `scripts/flows/validate_multi_agent_flow.py` ‚Üí Converter para teste automatizado
**Validar:**
- Integra√ß√£o natural entre agentes
- Contexto preservado durante transi√ß√µes

---

#### 6. `scripts/flows/validate_refinement_loop.py` ‚Üí Converter para teste automatizado
**Validar:**
- Refinamentos endere√ßam gaps de forma significativa
- Evolu√ß√£o √© coerente (n√£o apenas mudan√ßa cosm√©tica)

---

## üìä Estrat√©gia de Uso

### Fluxo T√≠pico de An√°lise

#### 1. Rodar Cen√°rio
```bash
python scripts/testing/run_scenario.py --scenario 3
```

#### 2. Revisar Output
Sistema printa relat√≥rio estruturado no terminal.

#### 3. Copiar para Claude (Se Necess√°rio)
Se problema identificado, copiar relat√≥rio e colar no Claude.

#### 4. Discuss√£o Contextualizada
Claude analisa, voc√™ discute, decidem pr√≥ximos passos juntos.

#### 5. Investigar Mais Fundo (Se Necess√°rio)
```bash
python scripts/testing/debug_scenario.py --scenario 3
```

Gera logs detalhados, copia para Claude, identifica causa raiz.

#### 6. Aplicar Corre√ß√£o
Claude sugere mudan√ßa no prompt ‚Üí voc√™ decide ‚Üí aplica via Cursor.

#### 7. Validar Corre√ß√£o
```bash
python scripts/testing/compare_results.py \
  --before baseline.json \
  --after current.json
```

Verifica se corre√ß√£o funcionou sem quebrar outros cen√°rios.

---

### Execu√ß√£o Local (√önica Forma Neste √âpico)

```bash
# Rodar apenas testes LLM-as-Judge
pytest -m llm_judge

# Rodar testes LLM-as-Judge + estruturais
pytest tests/integration/ -m "integration or llm_judge"

# Rodar com debug mode
pytest tests/integration/test_cenario_2.py --debug

# Rodar multi-turn completo
pytest tests/integration/test_multi_turn_flows.py
```

**Nota:** CI/CD ser√° considerado em √©picos futuros (√âpico 10+) quando houver contexto adequado (reposit√≥rio p√∫blico, m√∫ltiplos desenvolvedores, deploy cont√≠nuo).

---

### Quando Usar Cada Ferramenta

| Ferramenta | Quando Usar |
|------------|-------------|
| `run_scenario.py` | Validar cen√°rio espec√≠fico |
| `run_all_scenarios.py` | Validar suite completa |
| `debug_scenario.py` | Investigar problema espec√≠fico |
| `compare_results.py` | Validar mudan√ßa no prompt |
| `interactive_analyzer.py` | Explora√ß√£o geral / aprendizado |

---

## üí∞ Custo Estimado (Atualizado)

- **Por teste LLM-as-Judge:** ~$0.001-0.002 (Haiku)
- **Por cen√°rio multi-turn (3-5 turnos):** ~$0.005-0.010
- **Suite completa (10-15 testes single + 3-5 multi-turn):** ~$0.02-0.03
- **Execu√ß√£o semanal (desenvolvimento):** ~$0.10-0.15

**Nota:** Multi-turn aumenta custo mas √© essencial para validar fluxos completos.

**Comparado:**
- √âpico 7 manual: ~2-3h de trabalho
- √âpico 8 assistido: ~30-45min de trabalho
- **Economia:** 60-75% do tempo, custo similar

---

## üéØ Crit√©rios de Aceite do √âpico 8

### 8.1 Multi-Turn Executor Implementado (PRIORIDADE #1)
- [ ] `ConversationScenario` criado em `utils/test_scenarios.py`
- [ ] `MultiTurnExecutor` implementado em `utils/test_executor.py`
- [ ] Fixture `multi_turn_executor` criada em `tests/conftest.py`
- [ ] Cen√°rios 3, 6, 7 do √âpico 7 convertidos para multi-turn
- [ ] Testes em `tests/integration/test_multi_turn_flows.py` criados
- [ ] Valida√ß√£o estrutural + qualidade (LLM-as-Judge) funcionando

### 8.2 Debug Mode Implementado (PRIORIDADE #2)
- [ ] `DebugExecutor` implementado em `utils/test_executor.py`
- [ ] Fixture `debug_executor` criada em `tests/conftest.py`
- [ ] Logs salvos automaticamente em `logs/debug/`
- [ ] Compara√ß√£o antes/depois de mudan√ßas no prompt funcionando
- [ ] Flag `--debug` funciona em pytest

### 8.3 Infraestrutura LLM-as-Judge (PRIORIDADE #3)
- [ ] Fixture `llm_judge` criada em `tests/conftest.py`
- [ ] 5 prompts de avalia√ß√£o criados em `utils/test_prompts.py`
- [ ] Fun√ß√£o `extract_score` criada em `utils/test_helpers.py`
- [ ] Marker `@pytest.mark.llm_judge` adicionado em `pytest.ini`
- [ ] Testes pulam se `ANTHROPIC_API_KEY` n√£o est√° definida

### 8.4 Testes Automatizados Criados
- [ ] Testes multi-turn em `test_multi_turn_flows.py` (cen√°rios 3, 6, 7)
- [ ] Testes de qualidade em `test_multi_agent_smoke.py` (fluidez, integra√ß√£o)
- [ ] Testes de qualidade em `test_methodologist_smoke.py` (socr√°tico, decis√µes)
- [ ] Scripts de valida√ß√£o convertidos para testes automatizados:
  - [ ] `validate_socratic_behavior.py`
  - [ ] `validate_conversation_flow.py`
  - [ ] `validate_multi_agent_flow.py`
- [ ] Cada teste valida qualidade (score >= 4) al√©m de estrutura

### 8.5 Documenta√ß√£o Atualizada
- [ ] `docs/testing/epic8_automation_strategy.md` atualizado
- [ ] Aprendizados do √âpico 7 documentados
- [ ] Custos estimados atualizados (~$0.02-0.03 por execu√ß√£o)
- [ ] Estrat√©gia de execu√ß√£o local documentada
- [ ] Como adicionar novos testes documentado

**Removido do escopo:**
- ‚ùå CI/CD (postergar para √âpico 10+)

---

## üìö Refer√™ncias

- `docs/testing/epic7_results/summary.md` - Aprendizados do √âpico 7
- `docs/testing/epic7_validation_strategy.md` - Valida√ß√£o manual (Fase 1)
- `docs/roadmap_epic8_9_10.md` - Roadmap completo

---

**Vers√£o:** 2.0 (Reformulado ap√≥s √âpico 7)  
**Data:** Dezembro 2025  
**Filosofia:** Assistir an√°lise humana, n√£o substitu√≠-la
