# √âPICO 6: Melhorar Testes - Integra√ß√£o Real + Valida√ß√£o de Qualidade

> **Objetivo:** Resolver d√©bito t√©cnico: adicionar testes de integra√ß√£o reais onde h√° mocks superficiais e valida√ß√£o de qualidade conversacional com LLM-as-Judge.

---

## üìã Vis√£o Geral

**Problema atual:**
- Testes com mocks superficiais n√£o validam comportamento real (`test_orchestrator.py`, `test_structurer.py`)
- Testes verificam apenas estrutura (presen√ßa de campos), n√£o qualidade
- Comportamento socr√°tico imposs√≠vel de testar deterministicamente
- Sem garantia de que transi√ß√µes s√£o realmente "fluidas"
- Sem valida√ß√£o de que perguntas s√£o socr√°ticas vs burocr√°ticas

**Solu√ß√£o:**
1. **Adicionar testes de integra√ß√£o reais** onde h√° mocks superficiais (comportamento real)
2. **Implementar infraestrutura LLM-as-Judge** (valida√ß√£o de qualidade)
3. **ADICIONAR** valida√ß√£o de qualidade em 6 testes priorit√°rios (conforme `llm_judge_strategy.md`)
4. Manter testes unit√°rios existentes (estrutura) + adicionar camadas de valida√ß√£o

---

## üéØ Arquivos a Melhorar

### Fase 1: Adicionar Testes de Integra√ß√£o Reais

#### 1. `tests/unit/test_orchestrator.py` ‚Üí `tests/integration/test_orchestrator_integration.py`

**Problema atual:**
- Mocks retornam exatamente o esperado
- N√£o testa se LLM realmente classifica corretamente
- N√£o valida comportamento real

**Solu√ß√£o:**
- Criar `tests/integration/test_orchestrator_integration.py`
- Testes com API real validando classifica√ß√£o real
- Manter `test_orchestrator.py` (valida estrutura, mocks s√£o OK)

**Exemplo:**
```python
@pytest.mark.integration
def test_orchestrator_classifies_vague_input_real_api():
    """Testa classifica√ß√£o real com API (n√£o mock)."""
    state = create_initial_multi_agent_state(
        "Observei que desenvolver com IA √© mais r√°pido",
        session_id="test-real-1"
    )
    
    result = orchestrator_node(state)  # API real
    
    # Validar comportamento real
    assert result["next_step"] in ["explore", "clarify"]
    assert result["orchestrator_analysis"] is not None
    # Valida que LLM realmente classificou, n√£o apenas estrutura
```

#### 2. `tests/unit/test_structurer.py` ‚Üí `tests/integration/test_structurer_integration.py`

**Problema atual:**
- Mocks retornam exatamente o esperado
- N√£o testa se estrutura√ß√£o faz sentido

**Solu√ß√£o:**
- Criar `tests/integration/test_structurer_integration.py`
- Testes com API real validando estrutura√ß√£o real
- Manter `test_structurer.py` (valida estrutura, mocks s√£o OK)

---

### Fase 2: Adicionar Valida√ß√£o LLM-as-Judge

> **Nota:** Estes arquivos foram identificados em `docs/analysis/llm_judge_strategy.md` como candidatos priorit√°rios para LLM-as-Judge. O objetivo √© **ADICIONAR** valida√ß√£o de qualidade, n√£o refatorar completamente.

#### Prioridade ALTA (6 arquivos)

#### 1. `tests/integration/test_multi_agent_smoke.py`

**Problema atual:**
```python
def test_vague_idea_full_flow(multi_agent_graph):
    result = multi_agent_graph.invoke(state)
    assert result["orchestrator_analysis"] is not None  # ‚ùå Aceita qualquer coisa!
    assert result["next_step"] in ["explore", "suggest_agent", "clarify"]  # ‚ùå Muito fraco!
```

**Refatora√ß√£o:**
```python
@pytest.mark.llm_judge
def test_vague_idea_full_flow_quality(multi_agent_graph, llm_judge):
    """Valida qualidade da experi√™ncia conversacional end-to-end."""
    result = multi_agent_graph.invoke(state)
    
    # Valida√ß√£o estrutural (mant√©m)
    assert result["orchestrator_analysis"] is not None
    assert result["next_step"] in ["explore", "suggest_agent", "clarify"]
    
    # Valida√ß√£o de qualidade (NOVO)
    evaluation = llm_judge.invoke(
        CONVERSATION_QUALITY_PROMPT.format(
            response=result.get("messages", [])[-1].content if result.get("messages") else "",
            history=result.get("conversation_history", []),
            orchestrator_analysis=result.get("orchestrator_analysis", "")
        )
    )
    score = extract_score(evaluation.content)
    assert score >= 4, f"Experi√™ncia conversacional n√£o √© suficientemente fluida (score: {score})"
```

**O que validar:**
- Fluidez (sem "Posso chamar X?")
- Integra√ß√£o natural de outputs
- Confirma√ß√£o de entendimento

---

#### 2. `tests/integration/test_methodologist_smoke.py`

**Problema atual:**
```python
def test_methodologist_flow():
    result = methodologist_graph.invoke(state)
    assert result["status"] in ["approved", "rejected", "pending"]  # ‚ùå Aceita qualquer decis√£o!
```

**Adicionar valida√ß√£o LLM-as-Judge:**
```python
@pytest.mark.llm_judge
def test_methodologist_questions_quality(methodologist_graph, llm_judge):
    """Valida que perguntas s√£o socr√°ticas, n√£o burocr√°ticas."""
    result = methodologist_graph.invoke(state)
    
    if result.get("status") == "pending":
        # Validar qualidade das perguntas
        clarifications = result.get("clarifications", [])
        for question in clarifications:
            evaluation = llm_judge.invoke(
                SOCRATIC_QUESTION_PROMPT.format(question=question)
            )
            score = extract_score(evaluation.content)
            assert score >= 4, f"Pergunta n√£o √© suficientemente socr√°tica: {question} (score: {score})"
```

**O que validar:**
- Perguntas s√£o socr√°ticas (exp√µem assumptions) vs burocr√°ticas
- Timing natural (n√£o regras fixas)

---

#### 3. `scripts/flows/validate_socratic_behavior.py`

**Problema atual:**
- Valida apenas presen√ßa de palavras-chave (regex/contains)
- N√£o valida qualidade da provoca√ß√£o

**Adicionar valida√ß√£o LLM-as-Judge:**
```python
@pytest.mark.llm_judge
def test_socratic_provocation_quality():
    """Valida que provoca√ß√£o √© genuinamente socr√°tica."""
    result = orchestrator_node(state)
    
    evaluation = llm_judge.invoke(
        SOCRATIC_BEHAVIOR_PROMPT.format(
            response=result.get("messages", [])[-1].content,
            reflection_prompt=result.get("reflection_prompt", "")
        )
    )
    score = extract_score(evaluation.content)
    assert score >= 4, f"Provoca√ß√£o n√£o √© suficientemente socr√°tica (score: {score})"
```

**O que validar:**
- Provoca√ß√£o exp√µe assumptions (n√£o coleta burocr√°tica)
- Timing √© natural (n√£o regras fixas)
- Parada √© inteligente (n√£o insiste infinitamente)

---

#### 4. `scripts/flows/validate_conversation_flow.py`

**Problema atual:**
- Valida apenas regex/contains
- N√£o valida fluidez real

**Adicionar valida√ß√£o LLM-as-Judge:**
```python
@pytest.mark.llm_judge
def test_conversation_fluidity():
    """Valida fluidez conversacional (sem "Posso chamar X?")."""
    # Executar fluxo conversacional
    result = multi_agent_graph.invoke(state)
    
    # Validar que n√£o h√° perguntas de permiss√£o
    messages = result.get("messages", [])
    for msg in messages:
        if isinstance(msg, AIMessage):
            evaluation = llm_judge.invoke(
                FLUENCY_PROMPT.format(message=msg.content)
            )
            score = extract_score(evaluation.content)
            assert score >= 4, f"Mensagem n√£o √© suficientemente fluida: {msg.content[:50]}... (score: {score})"
```

**O que validar:**
- Sem "Posso chamar X?"
- Integra√ß√£o natural de outputs
- Confirma√ß√£o de entendimento

---

#### 5. `scripts/flows/validate_multi_agent_flow.py`

**Problema atual:**
- Valida apenas estrutura
- N√£o valida qualidade da integra√ß√£o

**Adicionar valida√ß√£o LLM-as-Judge:**
```python
@pytest.mark.llm_judge
def test_multi_agent_integration_quality():
    """Valida que transi√ß√µes entre agentes s√£o naturais."""
    result = multi_agent_graph.invoke(state)
    
    evaluation = llm_judge.invoke(
        INTEGRATION_QUALITY_PROMPT.format(
            orchestrator_output=result.get("orchestrator_analysis", ""),
            structurer_output=result.get("structurer_output", {}).get("structured_question", ""),
            methodologist_output=result.get("methodologist_output", {}).get("status", ""),
            messages=[msg.content for msg in result.get("messages", []) if isinstance(msg, AIMessage)]
        )
    )
    score = extract_score(evaluation.content)
    assert score >= 4, f"Integra√ß√£o entre agentes n√£o √© suficientemente natural (score: {score})"
```

**O que validar:**
- Transi√ß√µes s√£o naturais
- Contexto preservado
- Experi√™ncia coesa

---

#### 6. `scripts/flows/validate_refinement_loop.py`

**Problema atual:**
- Valida apenas que gaps foram endere√ßados
- N√£o valida qualidade das melhorias

**Adicionar valida√ß√£o LLM-as-Judge:**
```python
@pytest.mark.llm_judge
def test_refinement_quality():
    """Valida que refinamentos endere√ßam gaps de forma significativa."""
    # Executar loop de refinamento
    initial = structurer_node(state)
    refined = structurer_refinement_node(state)
    
    evaluation = llm_judge.invoke(
        REFINEMENT_QUALITY_PROMPT.format(
            initial_question=initial.get("structured_question", ""),
            refined_question=refined.get("structured_question", ""),
            gaps=initial.get("gaps", [])
        )
    )
    score = extract_score(evaluation.content)
    assert score >= 4, f"Refinamento n√£o endere√ßa gaps de forma significativa (score: {score})"
```

**O que validar:**
- Refinamentos endere√ßam gaps significativamente
- Evolu√ß√£o √© coerente

---

## üõ†Ô∏è Infraestrutura Necess√°ria

### 1. Fixture LLM-as-Judge (`tests/conftest.py`)

```python
@pytest.fixture
def llm_judge():
    """Fixture para LLM-as-judge (avaliador de qualidade)."""
    import os
    from langchain_anthropic import ChatAnthropic
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        pytest.skip("LLM-as-judge test skipped: ANTHROPIC_API_KEY not set")
    
    return ChatAnthropic(
        model="claude-3-5-haiku-20241022",  # Custo-benef√≠cio
        temperature=0  # Determin√≠stico
    )
```

### 2. Prompts de Avalia√ß√£o (`utils/test_prompts.py`)

```python
SOCRATIC_BEHAVIOR_PROMPT = """
Avalie se a resposta do sistema demonstra comportamento socr√°tico genu√≠no:

1. Provoca√ß√£o genu√≠na (exp√µe assumptions, n√£o coleta burocr√°tica)
2. Timing natural (n√£o regras fixas)
3. Parada inteligente (n√£o insiste infinitamente)

Resposta: {response}
Reflection prompt: {reflection_prompt}

Avalie de 1-5 (5 = excelente comportamento socr√°tico):
Justificativa:
"""

CONVERSATION_QUALITY_PROMPT = """
Avalie a qualidade da conversa√ß√£o:

1. Fluidez (sem "Posso chamar X?", integra√ß√£o natural)
2. Confirma√ß√£o de entendimento
3. Coer√™ncia com contexto

Resposta: {response}
Hist√≥rico: {history}
An√°lise do orquestrador: {orchestrator_analysis}

Avalie de 1-5 (5 = excelente experi√™ncia conversacional):
Justificativa:
"""

SOCRATIC_QUESTION_PROMPT = """
Avalie se a pergunta √© socr√°tica (exp√µe assumptions) ou burocr√°tica (coleta informa√ß√£o):

Pergunta: {question}

Avalie de 1-5 (5 = pergunta genuinamente socr√°tica):
Justificativa:
"""

FLUENCY_PROMPT = """
Avalie a fluidez da mensagem:

1. N√£o pergunta permiss√£o ("Posso chamar X?")
2. Integra√ß√£o natural
3. Tom conversacional

Mensagem: {message}

Avalie de 1-5 (5 = completamente fluida):
Justificativa:
"""

INTEGRATION_QUALITY_PROMPT = """
Avalie a qualidade da integra√ß√£o entre agentes:

1. Transi√ß√µes naturais
2. Contexto preservado
3. Experi√™ncia coesa

Orquestrador: {orchestrator_output}
Estruturador: {structurer_output}
Metodologista: {methodologist_output}
Mensagens: {messages}

Avalie de 1-5 (5 = integra√ß√£o excelente):
Justificativa:
"""

REFINEMENT_QUALITY_PROMPT = """
Avalie a qualidade do refinamento:

1. Endere√ßa gaps de forma significativa
2. Evolu√ß√£o coerente
3. Melhoria real (n√£o apenas mudan√ßa cosm√©tica)

Quest√£o inicial: {initial_question}
Quest√£o refinada: {refined_question}
Gaps identificados: {gaps}

Avalie de 1-5 (5 = refinamento excelente):
Justificativa:
"""
```

### 3. Fun√ß√£o Helper (`utils/test_helpers.py`)

```python
import re

def extract_score(evaluation_content: str) -> int:
    """Extrai score (1-5) da avalia√ß√£o do LLM-as-judge."""
    # Procura por padr√µes como "5", "score: 4", "Avalie de 1-5: 3"
    patterns = [
        r"Avalie de 1-5.*?(\d)",
        r"score.*?(\d)",
        r"(\d)\s*=\s*(?:excelente|√≥timo|bom)",
        r"^(\d)$"  # Apenas n√∫mero na linha
    ]
    
    for pattern in patterns:
        match = re.search(pattern, evaluation_content, re.IGNORECASE | re.MULTILINE)
        if match:
            score = int(match.group(1))
            if 1 <= score <= 5:
                return score
    
    raise ValueError(f"N√£o foi poss√≠vel extrair score v√°lido de: {evaluation_content}")
```

### 4. Marker no `pytest.ini`

```ini
[pytest]
markers =
    unit: Testes unit√°rios (mocks)
    integration: Testes de integra√ß√£o (API real)
    llm_judge: Testes que usam LLM-as-judge (requer API key)
```

---

## üìä Estrat√©gia de Execu√ß√£o

### Desenvolvimento Local
```bash
# Rodar apenas testes LLM-as-Judge
pytest -m llm_judge

# Rodar todos os testes (incluindo LLM-as-Judge)
pytest tests/
```

### Estrat√©gia de Execu√ß√£o

**Desenvolvimento Local:**
- Rodar seletivamente: `pytest -m llm_judge`
- Requer `ANTHROPIC_API_KEY` no ambiente
- Pode ser pulado (skip autom√°tico se chave n√£o estiver definida)

**CI/CD (futuro - n√£o implementado):**
- Atualmente n√£o h√° workflow para testes de integra√ß√£o
- Quando implementado: rodar apenas em PRs relevantes, usar chave de teste via GitHub Secrets

### Custo Estimado
- Por execu√ß√£o de teste LLM-as-Judge: ~$0.001-0.002 (usando Haiku)
- Suite completa (6 testes): ~$0.01-0.02 por execu√ß√£o

---

## ‚úÖ Crit√©rios de Aceite

### Testes de Integra√ß√£o Reais (6.1)
- [ ] Criar `tests/integration/test_orchestrator_integration.py` com testes de classifica√ß√£o real
- [ ] Criar `tests/integration/test_structurer_integration.py` com testes de estrutura√ß√£o real
- [ ] Testes devem usar API real (n√£o mocks)
- [ ] Testes devem validar comportamento real (n√£o apenas estrutura)
- [ ] Manter testes unit√°rios existentes (n√£o remover)

### Infraestrutura LLM-as-Judge (6.2)
- [ ] Fixture `llm_judge` criada em `tests/conftest.py`
- [ ] Prompts de avalia√ß√£o criados em `utils/test_prompts.py`
- [ ] Fun√ß√£o `extract_score` criada em `utils/test_helpers.py`
- [ ] Marker `@pytest.mark.llm_judge` adicionado em `pytest.ini`

### Valida√ß√£o de Qualidade (6.3)
- [ ] `test_multi_agent_smoke.py` - Adicionar valida√ß√£o de qualidade conversacional
- [ ] `test_methodologist_smoke.py` - Adicionar valida√ß√£o de perguntas socr√°ticas
- [ ] `validate_socratic_behavior.py` - Adicionar valida√ß√£o de provoca√ß√£o socr√°tica
- [ ] `validate_conversation_flow.py` - Adicionar valida√ß√£o de fluidez
- [ ] `validate_multi_agent_flow.py` - Adicionar valida√ß√£o de integra√ß√£o
- [ ] `validate_refinement_loop.py` - Adicionar valida√ß√£o de refinamento

### Documenta√ß√£o (6.4)
- [ ] Atualizar `docs/testing/strategy.md` com se√ß√£o sobre testes de integra√ß√£o reais e LLM-as-Judge
- [ ] Documentar custos estimados
- [ ] Documentar estrat√©gia de execu√ß√£o (local: `pytest -m integration`, `pytest -m llm_judge`)

---

## üìù Notas de Implementa√ß√£o

### Ordem de Implementa√ß√£o Recomendada

1. **Testes de integra√ß√£o reais primeiro** (6.1)
   - Criar `test_orchestrator_integration.py` e `test_structurer_integration.py`
   - Validar comportamento real (n√£o mocks)
   - Resolve d√©bito t√©cnico imediato

2. **Infraestrutura LLM-as-Judge** (6.2)
   - Criar fixture, prompts, helper
   - Testar com um teste simples antes de adicionar nos 6 arquivos

3. **Valida√ß√£o de qualidade** (6.3)
   - Come√ßar com `test_multi_agent_smoke.py` e `test_methodologist_smoke.py`
   - S√£o mais simples (j√° s√£o testes de integra√ß√£o)
   - **ADICIONAR** fun√ß√£o de teste com `@pytest.mark.llm_judge` (n√£o substituir teste existente)
   - Depois adicionar nos scripts de valida√ß√£o (itens 3-6)

### Manter Testes Existentes

- **N√ÉO remover** testes existentes (validam estrutura)
- **ADICIONAR** novos testes com valida√ß√£o de qualidade (LLM-as-Judge)
- Testes estruturais + testes de qualidade = cobertura completa

---

**Vers√£o:** 2.0  
**Data:** Dezembro 2025  
**Relacionado:** √âPICO 6 no ROADMAP

---

## üìù Nota sobre D√©bito T√©cnico

Este √©pico resolve d√©bito t√©cnico identificado na an√°lise de testes:
- **Mocks superficiais** ‚Üí Adicionar testes de integra√ß√£o reais (Fase 1)
- **Asserts fracos** ‚Üí Adicionar valida√ß√£o de qualidade (Fase 2)

**N√£o jogar para backlog:** Testes que n√£o agregam valor devem ser corrigidos ou removidos, n√£o ignorados.

