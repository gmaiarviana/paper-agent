"""
Script para debugar resposta do LLM especificamente na detecÃ§Ã£o de maturidade.
"""
import os
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import json
from agents.models.cognitive_model import CognitiveModel
from agents.persistence import detect_argument_maturity  # Usar mesma funÃ§Ã£o que validaÃ§Ã£o
from agents.persistence.snapshot_manager import MATURITY_DETECTION_PROMPT, SnapshotManager
from utils.config import create_anthropic_client
from langchain_core.messages import HumanMessage

def test_maturity_detection():
    """Testa detecÃ§Ã£o de maturidade com LLM."""
    print("=" * 70)
    print(" DEBUG: Testando DetecÃ§Ã£o de Maturidade com LLM")
    print("=" * 70)

    # Criar modelo cognitivo de teste (imaturo)
    model_v1 = CognitiveModel(
        claim="LLMs aumentam produtividade",
        premises=["Reduzem tempo de tarefas repetitivas"],
        assumptions=["UsuÃ¡rios sabem usar LLMs"],
        open_questions=["Quanto aumenta exatamente?"],
    )

    print("\nğŸ“Š Modelo Cognitivo V1 (imaturo):")
    print(f"   Claim: {model_v1.claim}")
    print(f"   Premises: {len(model_v1.premises)}")
    print(f"   Assumptions: {len(model_v1.assumptions)}")
    print(f"   Open questions: {len(model_v1.open_questions)}")

    # Preparar dados para LLM
    model_dict = model_v1.to_dict()
    model_json = json.dumps(model_dict, indent=2, ensure_ascii=False)

    print("\nğŸ“ JSON enviado ao LLM:")
    print(model_json[:200] + "..." if len(model_json) > 200 else model_json)

    # Preparar prompt
    prompt = MATURITY_DETECTION_PROMPT.format(cognitive_model_json=model_json)

    print(f"\nğŸ“ Tamanho do prompt: {len(prompt)} caracteres")
    print("\nğŸ“ Prompt (primeiros 500 chars):")
    print(prompt[:500] + "...")

    # Criar cliente LLM
    llm = create_anthropic_client("claude-3-5-haiku-20241022")
    print("\nâœ… Cliente LLM criado")

    # Invocar LLM
    try:
        print("\nğŸ“ Invocando LLM...")
        message = HumanMessage(content=prompt)
        response = llm.invoke([message])

        print(f"\nâœ… Resposta recebida (tipo: {type(response.content)})")
        print(f"ğŸ“ Tamanho da resposta: {len(response.content)} caracteres")
        print("\nğŸ“ Resposta completa:")
        print("=" * 70)
        print(response.content)
        print("=" * 70)

        # Tentar parsear JSON
        response_text = response.content.strip()

        print(f"\nğŸ” Resposta apÃ³s strip: '{response_text[:100]}...'")
        print(f"   ComeÃ§a com '```': {response_text.startswith('```')}")
        print(f"   ComeÃ§a com '{{': {response_text.startswith('{')}")

        # Remover markdown code blocks se presentes
        if response_text.startswith("```"):
            print("   â†’ Removendo markdown code block")
            lines = response_text.split("\n")
            response_text = "\n".join(lines[1:-1])
            print(f"   â†’ ApÃ³s remoÃ§Ã£o: '{response_text[:100]}...'")

        print("\nğŸ”§ Tentando parsear JSON...")
        assessment_dict = json.loads(response_text)
        print("âœ… JSON parseado com sucesso!")
        print(json.dumps(assessment_dict, indent=2, ensure_ascii=False))

    except json.JSONDecodeError as e:
        print(f"\nâŒ Erro ao parsear JSON: {e}")
        print(f"   PosiÃ§Ã£o do erro: linha {e.lineno}, coluna {e.colno}")
        print(f"   Mensagem: {e.msg}")
        print(f"\nğŸ“ Texto que tentou parsear:")
        print("=" * 70)
        print(repr(response_text))
        print("=" * 70)
        return False
    except Exception as e:
        print(f"\nâŒ Erro inesperado: {e}")
        import traceback
        traceback.print_exc()
        return False

    print("\n" + "=" * 70)
    print("âœ… TESTE PASSOU!")
    print("=" * 70)
    return True

def test_via_helper():
    """Testa usando a mesma funÃ§Ã£o helper que o validate_cognitive_model.py usa."""
    print("\n" + "=" * 70)
    print(" TESTE 2: Usando funÃ§Ã£o helper detect_argument_maturity()")
    print("=" * 70)

    # Criar modelo cognitivo de teste (imaturo)
    model_v1 = CognitiveModel(
        claim="LLMs aumentam produtividade",
        premises=["Reduzem tempo de tarefas repetitivas"],
        assumptions=["UsuÃ¡rios sabem usar LLMs"],
        open_questions=["Quanto aumenta exatamente?"],
    )

    print("\nğŸ“Š Modelo Cognitivo V1 (imaturo):")
    print(f"   Claim: {model_v1.claim}")
    print(f"   Premises: {len(model_v1.premises)}")

    print("\nğŸ“ Chamando detect_argument_maturity()...")
    try:
        assessment = detect_argument_maturity(model_v1)
        print(f"âœ… AvaliaÃ§Ã£o recebida:")
        print(f"   Maduro: {assessment.is_mature}")
        print(f"   ConfianÃ§a: {assessment.confidence:.2f}")
        print(f"   Justificativa: {assessment.justification[:100]}...")
        if assessment.missing_elements:
            print(f"   Elementos faltando: {', '.join(assessment.missing_elements)}")

        print("\n" + "=" * 70)
        print("âœ… TESTE PASSOU! LLM funcionando via helper")
        print("=" * 70)
        return True
    except Exception as e:
        print(f"\nâŒ Erro ao chamar helper: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("EXECUTANDO DOIS TESTES:\n")
    print("1. Teste direto com LLM (para ver resposta completa)")
    print("2. Teste via helper detect_argument_maturity() (mesmo que validaÃ§Ã£o usa)")

    success1 = test_maturity_detection()
    success2 = test_via_helper()

    sys.exit(0 if (success1 and success2) else 1)
