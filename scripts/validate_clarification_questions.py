#!/usr/bin/env python
"""
Script de valida√ß√£o das Consultas Inteligentes (√âpico 14).

Este script verifica se todos os componentes do √âpico 14 est√£o funcionando:
1. Identifica√ß√£o de necessidades de esclarecimento (14.1)
2. Gera√ß√£o de perguntas sobre contradi√ß√µes (14.3)
3. Sugest√£o de perguntas sobre gaps (14.4)
4. Decis√£o de timing de interven√ß√£o (14.5)
5. An√°lise de resposta de esclarecimento (14.6)

Uso:
    python scripts/validate_clarification_questions.py

Sa√≠da esperada:
    ‚úÖ Todos os testes passaram!

Nota:
    Este script requer depend√™ncias do projeto. Para executar em ambiente limpo,
    instale as depend√™ncias primeiro: pip install -r requirements.txt
"""

import sys
import time
import logging
from pathlib import Path

# Adicionar diret√≥rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)

# Flag para verificar se depend√™ncias est√£o dispon√≠veis
DEPS_AVAILABLE = True
try:
    from unittest.mock import patch, MagicMock
    import pydantic
except ImportError:
    DEPS_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Depend√™ncias n√£o dispon√≠veis. Executando valida√ß√£o estrutural apenas.")

def validate_imports():
    """Valida que todos os m√≥dulos podem ser importados."""
    logger.info("\nüì¶ Validando imports...")

    errors = []

    # Fun√ß√µes de clarification
    try:
        from agents.observer.clarification import (
            identify_clarification_needs,
            generate_contradiction_question,
            suggest_question_for_gap,
            should_ask_clarification,
            analyze_clarification_response,
            update_clarification_persistence,
            get_clarification_summary_for_timeline
        )
        logger.info("  ‚úÖ Fun√ß√µes de clarification importadas")
    except ImportError as e:
        errors.append(f"  ‚ùå Falha ao importar fun√ß√µes de clarification: {e}")

    # Modelos Pydantic
    try:
        from agents.models.clarification import (
            ClarificationNeed,
            ClarificationContext,
            ClarificationTimingDecision,
            ClarificationResponse,
            ClarificationUpdates,
            QuestionSuggestion
        )
        logger.info("  ‚úÖ Modelos Pydantic de clarification importados")
    except ImportError as e:
        errors.append(f"  ‚ùå Falha ao importar modelos de clarification: {e}")

    # Eventos
    try:
        from core.utils.event_models import (
            ClarificationRequestedEvent,
            ClarificationResolvedEvent
        )
        logger.info("  ‚úÖ Eventos de clarification importados")
    except ImportError as e:
        errors.append(f"  ‚ùå Falha ao importar eventos de clarification: {e}")

    # Prompts
    try:
        from agents.observer.clarification_prompts import (
            IDENTIFY_CLARIFICATION_NEEDS_PROMPT,
            CONTRADICTION_QUESTION_PROMPT,
            GAP_QUESTION_PROMPT,
            ANALYZE_CLARIFICATION_RESPONSE_PROMPT
        )
        logger.info("  ‚úÖ Prompts de clarification importados")
    except ImportError as e:
        errors.append(f"  ‚ùå Falha ao importar prompts de clarification: {e}")

    if errors:
        for err in errors:
            logger.error(err)
        return False

    return True

def validate_models():
    """Valida cria√ß√£o e serializa√ß√£o dos modelos Pydantic."""
    logger.info("\nüìã Validando modelos Pydantic...")

    from agents.models.clarification import (
        ClarificationNeed,
        ClarificationContext,
        ClarificationTimingDecision,
        ClarificationResponse,
        ClarificationUpdates,
        QuestionSuggestion
    )

    errors = []

    # ClarificationNeed
    try:
        need = ClarificationNeed(
            needs_clarification=True,
            clarification_type="contradiction",
            description="Tens√£o entre proposi√ß√µes X e Y",
            suggested_approach="Explorar contextos diferentes",
            priority="high",
            turn_detected=5,
            turns_persisted=2
        )
        assert need.id is not None  # UUID gerado
        assert need.needs_clarification is True
        assert need.priority == "high"

        # Testar serializa√ß√£o
        data = need.to_dict()
        need_restored = ClarificationNeed.from_dict(data)
        assert need_restored.clarification_type == "contradiction"
        logger.info("  ‚úÖ ClarificationNeed: cria√ß√£o e serializa√ß√£o OK")
    except Exception as e:
        errors.append(f"  ‚ùå ClarificationNeed: {e}")

    # ClarificationContext
    try:
        context = ClarificationContext(
            proposicoes=["LLMs aumentam produtividade", "LLMs aumentam bugs"],
            contradictions=["Produtividade vs bugs"],
            open_questions=["Em que contexto?"],
            claim_excerpt="LLMs impactam desenvolvimento"
        )
        assert len(context.proposicoes) == 2
        logger.info("  ‚úÖ ClarificationContext: cria√ß√£o OK")
    except Exception as e:
        errors.append(f"  ‚ùå ClarificationContext: {e}")

    # ClarificationTimingDecision
    try:
        decision = ClarificationTimingDecision(
            should_ask=True,
            reason="Contradi√ß√£o persiste h√° 3 turnos",
            delay_turns=0,
            urgency="high"
        )
        assert decision.should_ask is True
        logger.info("  ‚úÖ ClarificationTimingDecision: cria√ß√£o OK")
    except Exception as e:
        errors.append(f"  ‚ùå ClarificationTimingDecision: {e}")

    # ClarificationResponse
    try:
        updates = ClarificationUpdates(
            proposicoes_to_add=["Nova proposi√ß√£o"],
            contradictions_to_resolve=[0],
            open_questions_to_close=[1]
        )
        response = ClarificationResponse(
            resolution_status="resolved",
            summary="Usu√°rio esclareceu os contextos",
            updates=updates,
            needs_followup=False
        )
        assert response.resolution_status == "resolved"
        logger.info("  ‚úÖ ClarificationResponse: cria√ß√£o OK")
    except Exception as e:
        errors.append(f"  ‚ùå ClarificationResponse: {e}")

    # QuestionSuggestion
    try:
        suggestion = QuestionSuggestion(
            question_text="Voc√™ mencionou X e Y. Eles se aplicam em situa√ß√µes diferentes?",
            target_type="contradiction",
            related_proposicoes=["X", "Y"],
            expected_outcome="Esclarecimento do contexto",
            tone_guidance="Curiosidade genu√≠na"
        )
        assert suggestion.target_type == "contradiction"
        logger.info("  ‚úÖ QuestionSuggestion: cria√ß√£o OK")
    except Exception as e:
        errors.append(f"  ‚ùå QuestionSuggestion: {e}")

    if errors:
        for err in errors:
            logger.error(err)
        return False

    return True

def validate_timing_logic():
    """Valida l√≥gica de timing de interven√ß√£o (14.5)."""
    logger.info("\n‚è±Ô∏è Validando l√≥gica de timing (14.5)...")

    from agents.observer.clarification import should_ask_clarification
    from agents.models.clarification import ClarificationNeed

    checks = []

    # Cen√°rio 1: Sem necessidade de esclarecimento
    need_none = ClarificationNeed(
        needs_clarification=False,
        clarification_type="confusion",
        description="Conversa fluindo bem"
    )
    decision = should_ask_clarification(need_none, [], current_turn=5)
    checks.append(("Sem necessidade ‚Üí n√£o perguntar", decision.should_ask is False))

    # Cen√°rio 2: Prioridade alta sempre pergunta
    need_high = ClarificationNeed(
        needs_clarification=True,
        clarification_type="contradiction",
        description="Contradi√ß√£o cr√≠tica",
        suggested_approach="Perguntar",
        priority="high"
    )
    decision = should_ask_clarification(need_high, [], current_turn=5, turns_since_last_question=3)
    checks.append(("Prioridade alta ‚Üí perguntar", decision.should_ask is True))
    checks.append(("Prioridade alta ‚Üí urg√™ncia high", decision.urgency == "high"))

    # Cen√°rio 3: Contradi√ß√£o persistente (2+ turnos)
    need_persist = ClarificationNeed(
        needs_clarification=True,
        clarification_type="contradiction",
        description="Contradi√ß√£o entre X e Y",
        suggested_approach="Explorar",
        priority="medium",
        turns_persisted=3
    )
    decision = should_ask_clarification(need_persist, [], current_turn=5, turns_since_last_question=5)
    checks.append(("Contradi√ß√£o persistente ‚Üí perguntar", decision.should_ask is True))

    # Cen√°rio 4: Pergunta recente - deve esperar
    need_recent = ClarificationNeed(
        needs_clarification=True,
        clarification_type="gap",
        description="Gap",
        suggested_approach="Perguntar",
        priority="medium"
    )
    decision = should_ask_clarification(need_recent, [], current_turn=5, turns_since_last_question=1)
    checks.append(("Pergunta recente ‚Üí esperar", decision.should_ask is False))
    checks.append(("Pergunta recente ‚Üí delay > 0", decision.delay_turns > 0))

    # Cen√°rio 5: Usu√°rio fluindo bem - n√£o interromper
    need_flow = ClarificationNeed(
        needs_clarification=True,
        clarification_type="gap",
        description="Gap menor",
        suggested_approach="Eventualmente",
        priority="low",
        turns_persisted=1
    )
    decision = should_ask_clarification(
        need_flow, [], current_turn=5,
        turns_since_last_question=5, is_user_flowing=True
    )
    checks.append(("Usu√°rio fluindo ‚Üí n√£o interromper", decision.should_ask is False))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

def validate_persistence_update():
    """Valida atualiza√ß√£o de persist√™ncia de necessidade (14.5)."""
    logger.info("\nüîÑ Validando atualiza√ß√£o de persist√™ncia...")

    from agents.observer.clarification import update_clarification_persistence
    from agents.models.clarification import ClarificationNeed

    checks = []

    # Incrementar persist√™ncia
    need = ClarificationNeed(
        needs_clarification=True,
        clarification_type="contradiction",
        description="Contradi√ß√£o",
        suggested_approach="Perguntar",
        turns_persisted=2
    )
    updated = update_clarification_persistence(need, still_relevant=True)
    checks.append(("Incrementar persist√™ncia", updated.turns_persisted == 3))

    # Resetar quando n√£o relevante
    need2 = ClarificationNeed(
        needs_clarification=True,
        clarification_type="contradiction",
        description="Contradi√ß√£o",
        suggested_approach="Perguntar",
        turns_persisted=5
    )
    updated2 = update_clarification_persistence(need2, still_relevant=False)
    checks.append(("Reset quando n√£o relevante", updated2.turns_persisted == 0))
    checks.append(("Marcar como n√£o necess√°rio", updated2.needs_clarification is False))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

def validate_timeline_summary():
    """Valida gera√ß√£o de resumo para timeline (14.6)."""
    logger.info("\nüìú Validando resumo para timeline...")

    from agents.observer.clarification import get_clarification_summary_for_timeline
    from agents.models.clarification import ClarificationResponse, ClarificationUpdates

    checks = []

    # Esclarecimento resolvido
    response_resolved = ClarificationResponse(
        resolution_status="resolved",
        summary="Usu√°rio explicou os contextos diferentes",
        updates=ClarificationUpdates(),
        needs_followup=False
    )
    summary = get_clarification_summary_for_timeline(response_resolved, "contradiction")
    checks.append(("Resolved ‚Üí emoji ‚úÖ", "‚úÖ" in summary))
    checks.append(("Resolved ‚Üí 'esclarecida'", "esclarecida" in summary.lower()))

    # Parcialmente resolvido
    response_partial = ClarificationResponse(
        resolution_status="partially_resolved",
        summary="Algumas d√∫vidas permanecem",
        updates=ClarificationUpdates(),
        needs_followup=True
    )
    summary = get_clarification_summary_for_timeline(response_partial, "gap")
    checks.append(("Partial ‚Üí 'parcialmente'", "parcialmente" in summary.lower()))

    # N√£o resolvido
    response_unresolved = ClarificationResponse(
        resolution_status="unresolved",
        summary="Resposta tangencial",
        updates=ClarificationUpdates(),
        needs_followup=False
    )
    summary = get_clarification_summary_for_timeline(response_unresolved, "confusion")
    checks.append(("Unresolved ‚Üí 'pendente'", "pendente" in summary.lower()))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

def validate_events():
    """Valida cria√ß√£o de eventos de clarification."""
    logger.info("\nüì° Validando eventos de clarification...")

    from core.utils.event_models import ClarificationRequestedEvent, ClarificationResolvedEvent

    checks = []

    # ClarificationRequestedEvent
    try:
        event_req = ClarificationRequestedEvent(
            session_id="test-session",
            turn_number=5,
            clarification_type="contradiction",
            question="Voc√™ mencionou X e Y. Eles se aplicam em contextos diferentes?",
            priority="medium",
            related_context={"proposicoes": ["X", "Y"]}
        )
        checks.append(("ClarificationRequestedEvent criado", True))
        checks.append(("event_type correto", event_req.event_type == "clarification_requested"))
        checks.append(("turn_number correto", event_req.turn_number == 5))
    except Exception as e:
        checks.append((f"ClarificationRequestedEvent: {e}", False))

    # ClarificationResolvedEvent
    try:
        event_res = ClarificationResolvedEvent(
            session_id="test-session",
            turn_number=6,
            clarification_type="contradiction",
            resolution_status="resolved",
            summary="Usu√°rio esclareceu os contextos",
            updates_made={"contradictions_resolved": 1}
        )
        checks.append(("ClarificationResolvedEvent criado", True))
        checks.append(("event_type correto", event_res.event_type == "clarification_resolved"))
        checks.append(("resolution_status correto", event_res.resolution_status == "resolved"))
    except Exception as e:
        checks.append((f"ClarificationResolvedEvent: {e}", False))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

@patch('agents.observer.clarification.invoke_with_retry')
@patch('agents.observer.clarification._get_llm')
def validate_identify_needs_with_mock(mock_get_llm, mock_invoke):
    """Valida identify_clarification_needs com mock do LLM (14.1)."""
    logger.info("\nüîç Validando identifica√ß√£o de necessidades com mock (14.1)...")

    from agents.observer.clarification import identify_clarification_needs

    checks = []

    # Mock da resposta do LLM para contradi√ß√£o
    mock_response = MagicMock()
    mock_response.content = '''```json
    {
        "needs_clarification": true,
        "clarification_type": "contradiction",
        "description": "Usu√°rio disse X e Y que parecem contradit√≥rios",
        "relevant_context": {
            "proposicoes": ["X", "Y"],
            "contradictions": ["X vs Y"]
        },
        "suggested_approach": "Explorar contextos diferentes",
        "priority": "high"
    }
    ```'''
    mock_invoke.return_value = mock_response

    cognitive_model = {
        "claim": "LLMs impactam desenvolvimento",
        "proposicoes": [
            {"texto": "LLMs aumentam produtividade", "solidez": 0.7},
            {"texto": "LLMs aumentam bugs", "solidez": 0.6}
        ],
        "contradictions": [{"description": "Produtividade vs bugs", "confidence": 0.85}],
        "open_questions": []
    }

    need = identify_clarification_needs(cognitive_model, turn_number=5)

    checks.append(("Retorna ClarificationNeed", hasattr(need, 'needs_clarification')))
    checks.append(("needs_clarification=True", need.needs_clarification is True))
    checks.append(("clarification_type=contradiction", need.clarification_type == "contradiction"))
    checks.append(("priority=high", need.priority == "high"))
    checks.append(("turn_detected=5", need.turn_detected == 5))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

@patch('agents.observer.clarification.invoke_with_retry')
@patch('agents.observer.clarification._get_llm')
def validate_contradiction_question_with_mock(mock_get_llm, mock_invoke):
    """Valida generate_contradiction_question com mock do LLM (14.3)."""
    logger.info("\n‚ùì Validando gera√ß√£o de pergunta sobre contradi√ß√£o com mock (14.3)...")

    from agents.observer.clarification import generate_contradiction_question

    checks = []

    # Mock da resposta do LLM
    mock_response = MagicMock()
    mock_response.content = '''```json
    {
        "question": "Voc√™ mencionou que LLMs aumentam produtividade e tamb√©m aumentam bugs. Esses efeitos acontecem em contextos diferentes?",
        "expected_outcomes": [
            "Esclarecimento sobre contextos espec√≠ficos",
            "Diferencia√ß√£o de cen√°rios"
        ],
        "tone_check": "Curiosidade genu√≠na, sem julgamento"
    }
    ```'''
    mock_invoke.return_value = mock_response

    contradiction = {
        "description": "LLMs aumentam produtividade vs LLMs aumentam bugs",
        "confidence": 0.85
    }
    propositions = [
        {"texto": "LLMs aumentam produtividade em 30%", "solidez": 0.7},
        {"texto": "LLMs aumentam quantidade de bugs", "solidez": 0.6}
    ]
    context = "Usu√°rio est√° desenvolvendo argumento sobre impacto de LLMs"

    suggestion = generate_contradiction_question(contradiction, propositions, context)

    checks.append(("Retorna QuestionSuggestion", hasattr(suggestion, 'question_text')))
    checks.append(("question_text n√£o vazio", len(suggestion.question_text) > 0))
    checks.append(("target_type=contradiction", suggestion.target_type == "contradiction"))
    checks.append(("tone_guidance presente", len(suggestion.tone_guidance) > 0))

    # Validar que pergunta √© contextual (menciona conceitos da conversa)
    question_lower = suggestion.question_text.lower()
    is_contextual = "produtividade" in question_lower or "bugs" in question_lower or "llm" in question_lower
    checks.append(("Pergunta √© contextual", is_contextual))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

@patch('agents.observer.clarification.invoke_with_retry')
@patch('agents.observer.clarification._get_llm')
def validate_gap_question_with_mock(mock_get_llm, mock_invoke):
    """Valida suggest_question_for_gap com mock do LLM (14.4)."""
    logger.info("\nüï≥Ô∏è Validando sugest√£o de pergunta sobre gap com mock (14.4)...")

    from agents.observer.clarification import suggest_question_for_gap

    checks = []

    # Mock da resposta do LLM
    mock_response = MagicMock()
    mock_response.content = '''```json
    {
        "question": "Voc√™ tem algum dado ou experi√™ncia que demonstre esse aumento de produtividade?",
        "connection_to_claim": "Evid√™ncia emp√≠rica fortaleceria o argumento central"
    }
    ```'''
    mock_invoke.return_value = mock_response

    cognitive_model = {
        "claim": "LLMs aumentam produtividade em 30%",
        "proposicoes": [{"texto": "Equipes de tech usam LLMs", "solidez": 0.6}],
        "open_questions": [
            "Qual baseline de compara√ß√£o?",
            "Qual a metodologia de medi√ß√£o?"
        ],
        "contradictions": []
    }

    suggestion = suggest_question_for_gap(cognitive_model, gap_index=0, conversation_context="Discuss√£o sobre impacto de LLMs")

    checks.append(("Retorna QuestionSuggestion", suggestion is not None))
    if suggestion:
        checks.append(("question_text n√£o vazio", len(suggestion.question_text) > 0))
        checks.append(("target_type=gap", suggestion.target_type == "gap"))
        checks.append(("expected_outcome presente", len(suggestion.expected_outcome) > 0))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

@patch('agents.observer.clarification.invoke_with_retry')
@patch('agents.observer.clarification._get_llm')
def validate_analyze_response_with_mock(mock_get_llm, mock_invoke):
    """Valida analyze_clarification_response com mock do LLM (14.6)."""
    logger.info("\nüìä Validando an√°lise de resposta com mock (14.6)...")

    from agents.observer.clarification import analyze_clarification_response
    from agents.models.clarification import ClarificationNeed

    checks = []

    # Mock da resposta do LLM
    mock_response = MagicMock()
    mock_response.content = '''```json
    {
        "resolution_status": "resolved",
        "summary": "Usu√°rio esclareceu que produtividade aumenta em tarefas simples enquanto bugs aumentam em tarefas complexas",
        "updates": {
            "proposicoes_to_add": ["Produtividade aumenta em tarefas simples", "Bugs aumentam em tarefas complexas"],
            "contradictions_to_resolve": [0],
            "context_to_add": {"task_complexity": "fator diferenciador"}
        },
        "needs_followup": false
    }
    ```'''
    mock_invoke.return_value = mock_response

    original_need = ClarificationNeed(
        needs_clarification=True,
        clarification_type="contradiction",
        description="Produtividade vs bugs"
    )

    cognitive_model = {
        "claim": "LLMs impactam desenvolvimento",
        "proposicoes": [],
        "contradictions": [{"description": "Produtividade vs bugs"}],
        "open_questions": []
    }

    response = analyze_clarification_response(
        user_response="Na verdade, produtividade aumenta em tarefas simples, mas bugs aumentam quando a tarefa √© complexa",
        question_asked="Esses efeitos acontecem em contextos diferentes?",
        original_need=original_need,
        cognitive_model=cognitive_model
    )

    checks.append(("Retorna ClarificationResponse", hasattr(response, 'resolution_status')))
    checks.append(("resolution_status=resolved", response.resolution_status == "resolved"))
    checks.append(("summary n√£o vazio", len(response.summary) > 0))
    checks.append(("updates presente", response.updates is not None))
    checks.append(("needs_followup=False", response.needs_followup is False))

    # Verificar updates
    if response.updates:
        checks.append(("proposicoes_to_add populado", len(response.updates.proposicoes_to_add) > 0))
        checks.append(("contradictions_to_resolve populado", len(response.updates.contradictions_to_resolve) > 0))

    all_passed = True
    for name, passed in checks:
        if passed:
            logger.info(f"  ‚úÖ {name}")
        else:
            logger.error(f"  ‚ùå {name}")
            all_passed = False

    return all_passed

def run_unit_tests():
    """Executa testes unit√°rios relacionados ao √âpico 14."""
    logger.info("\nüß™ Executando testes unit√°rios...")

    import subprocess

    test_file = "tests/unit/test_clarification.py"

    if Path(test_file).exists():
        result = subprocess.run(
            ["python", "-m", "pytest", test_file, "-v", "--tb=short", "-q"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            # Contar testes passados
            output = result.stdout
            logger.info(f"  ‚úÖ {test_file}")
            # Extrair resumo
            for line in output.split('\n'):
                if 'passed' in line:
                    logger.info(f"     {line.strip()}")
            return True
        else:
            logger.error(f"  ‚ùå {test_file}")
            logger.error(f"     {result.stdout[-500:] if result.stdout else result.stderr[-500:]}")
            return False
    else:
        logger.warning(f"  ‚ö†Ô∏è {test_file} n√£o encontrado")
        return True  # N√£o falhar se arquivo n√£o existe

def validate_file_structure():
    """Valida que todos os arquivos necess√°rios existem."""
    logger.info("\nüìÅ Validando estrutura de arquivos...")

    required_files = [
        "agents/observer/clarification.py",
        "agents/observer/clarification_prompts.py",
        "agents/models/clarification.py",
        "utils/event_models.py",
        "tests/unit/test_clarification.py",
        "app/components/backstage/timeline.py"  # Timeline com se√ß√£o de clarification
    ]

    base_path = Path(__file__).parent.parent
    all_exist = True

    for file in required_files:
        path = base_path / file
        if path.exists():
            logger.info(f"  ‚úÖ {file}")
        else:
            logger.error(f"  ‚ùå {file} n√£o encontrado")
            all_exist = False

    return all_exist

def main():
    """Executa todas as valida√ß√µes."""
    logger.info("=" * 60)
    logger.info("üîç VALIDA√á√ÉO DO √âPICO 14 - Observer Consultas Inteligentes")
    logger.info("=" * 60)

    # Sempre validar estrutura de arquivos
    results = {
        "file_structure": validate_file_structure()
    }

    # Se depend√™ncias est√£o dispon√≠veis, executar valida√ß√µes completas
    if DEPS_AVAILABLE:
        results.update({
            "imports": validate_imports(),
            "models": validate_models(),
            "timing_logic": validate_timing_logic(),
            "persistence_update": validate_persistence_update(),
            "timeline_summary": validate_timeline_summary(),
            "events": validate_events(),
            "identify_needs_mock": validate_identify_needs_with_mock(),
            "contradiction_question_mock": validate_contradiction_question_with_mock(),
            "gap_question_mock": validate_gap_question_with_mock(),
            "analyze_response_mock": validate_analyze_response_with_mock(),
            "unit_tests": run_unit_tests()
        })
    else:
        logger.info("\n‚ö†Ô∏è Valida√ß√£o estrutural apenas (depend√™ncias n√£o dispon√≠veis)")
        logger.info("   Para valida√ß√£o completa, instale: pip install -r requirements.txt")

    # Resumo
    logger.info("\n" + "=" * 60)
    logger.info("üìã RESUMO DA VALIDA√á√ÉO")
    logger.info("=" * 60)

    passed = sum(results.values())
    total = len(results)

    for name, result in results.items():
        status = "‚úÖ" if result else "‚ùå"
        logger.info(f"  {status} {name}")

    logger.info("")

    if passed == total:
        logger.info(f"üéâ SUCESSO: Todos os {total} testes passaram!")
        logger.info("")
        if DEPS_AVAILABLE:
            logger.info("√âpico 14 - Componentes validados:")
            logger.info("  - 14.1: Identifica√ß√£o de necessidades ‚úÖ")
            logger.info("  - 14.3: Perguntas sobre contradi√ß√µes ‚úÖ")
            logger.info("  - 14.4: Perguntas sobre gaps ‚úÖ")
            logger.info("  - 14.5: Timing de interven√ß√£o ‚úÖ")
            logger.info("  - 14.6: An√°lise de resposta ‚úÖ")
            logger.info("")
            logger.info("‚ö†Ô∏è Pendente (depende do √âpico 13):")
            logger.info("  - 14.2: Integra√ß√£o com Orquestrador")
        else:
            logger.info("√âpico 14 - Estrutura validada:")
            logger.info("  - Todos os arquivos necess√°rios existem ‚úÖ")
        return 0
    else:
        logger.error(f"‚ö†Ô∏è FALHA: {total - passed} de {total} testes falharam")
        return 1

if __name__ == "__main__":
    sys.exit(main())
